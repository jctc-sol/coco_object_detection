{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from model   import SSD300\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "image batch tensor shape: torch.Size([8, 3, 300, 300])\n",
      "bounding box location prediction shape: torch.Size([8, 8732, 4])\n",
      "object class prediction shape: torch.Size([8, 8732, 81])\n"
     ]
    }
   ],
   "source": [
    "# define the sequence of transformations to apply to each image sample \n",
    "basic_tfs = [PhotometricDistort(1.),\n",
    "             Flip(0.5),\n",
    "             ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "             Zoomout(0.5, max_scale=2.5),\n",
    "             Normalize(), \n",
    "             Resize((300,300))]\n",
    "tfms = transforms.Compose(basic_tfs)\n",
    "\n",
    "# instantiate the dataset object\n",
    "ds = CocoDataset(data_dir='./', dataset='val2017', anno_type='instances', transforms=tfms)\n",
    "\n",
    "# create dataloader\n",
    "BS = 8\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True, \n",
    "                collate_fn=partial(ds.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape\n",
    "\n",
    "# create model object\n",
    "ssd = SSD300(len(ds.id2cat))\n",
    "\n",
    "# test forward pass for one batch\n",
    "for batch in dl:\n",
    "    image_batch = batch['images']\n",
    "    print(f\"image batch tensor shape: {image_batch.size()}\")\n",
    "    # forward pass through SSD300\n",
    "    pred_boxes, pred_scores = ssd(image_batch)\n",
    "    print(f\"bounding box location prediction shape: {pred_boxes.size()}\")\n",
    "    print(f\"object class prediction shape: {pred_scores.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Over Union (IoU)\n",
    "\n",
    "The IoU is a simple concept that compute the \"intersection over union\" of two bounding box regions $b1$ and $b2$. The union is computed as the sum of areas of the two boxes together minus the overlaping area (intersection) of the two boxes. THe IoU is then simply computed as a ratio of $\\frac{b1 \\cap b2}{b1 \\cup b2}$. This is also known as the **Jaccard overlap**.\n",
    "\n",
    "\n",
    "This requires two separate computations given two sets of center coordinates $(x_{c1}, y_{c1}, w_{c1}, h_{c1})$ and $(x_{c2}, y_{c2}, w_{c2}, h_{c2})$: \n",
    "\n",
    "1) intersection of area $A_{b1 \\cap b2} = b1 \\cap b2$; and \n",
    "\n",
    "2) overlap of area: $A_{b1 \\cup b2} = A_{b1} + A_{b2} - A_{b1 \\cap b2}$\n",
    "\n",
    "## Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the intersection of every box combination between two sets of boxes that are in \n",
    "    boundary coordinates (x_1, y_1, x_2, y_2)\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4) in boundary coordinates\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4) in boundary coordinates\n",
    "    :return: intersection of each of the boxes in set 1 with respect to each of the boxes \n",
    "             in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"    \n",
    "    # Following code from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jaccard_overlap(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \n",
    "    Code from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    \"\"\"\n",
    "    # convert from center coordinates to boundary coordinates\n",
    "    bcoord = BoundaryCoord()\n",
    "    set_1 = bcoord.encode(set_1)\n",
    "    set_2 = bcoord.encode(set_2)\n",
    "    \n",
    "    # Find intersections\n",
    "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
    "\n",
    "    # Find areas of each box in both sets\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
    "\n",
    "    # Find the union\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
    "\n",
    "    return intersection / union  # (n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single dataset sample\n",
    "sample = ds[0]\n",
    "_, h, w = sample['image'].size()\n",
    "bboxes = sample['boxes']\n",
    "\n",
    "# instantiate transform\n",
    "ccoord = Coco2CenterCoord(w, h)\n",
    "\n",
    "# encode bounding boxes in center coords\n",
    "bboxes = ccoord.encode(bboxes)\n",
    "# get prior bounding boxes from SSD (which is already constructed in center coordinates)\n",
    "prior_boxes = ssd.prior_boxes\n",
    "\n",
    "# compute Jaccard overlap\n",
    "jOverlap = find_jaccard_overlap(bboxes, prior_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 out of 8732 prior bounding boxes have overlap region with threshold > 0.5\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "print(f\"{torch.sum(jOverlap > threshold)} out of {prior_boxes.shape[0]} \\\n",
    "prior bounding boxes have overlap region with threshold > {threshold}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mAP():\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        \"\"\"\n",
    "        :param n_classes: number of class objects to compute mean AP over; note that 0 \n",
    "                          should be reserved as background class\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        \n",
    "    def concat_batch_tensors(boxes, labels, scores=None):\n",
    "        \"\"\"\n",
    "        Each batch contains M images, and each image contains N_i objects within (since each object contains\n",
    "        different numbr of objects). As a result, each of boxes, labels, scores are in the form of list of \n",
    "        tensors. This helper function simply concatenates all tensors within the list into a single one for\n",
    "        the batch.\n",
    "        \n",
    "        :param boxes: list of M tensors each of size (N_i, 4) for bounding boxes of each image within the batch\n",
    "        :param labels: list of M tensors each of size (N_i, self.n_class) for labels of objects within each image within the batch\n",
    "        :param scores: list of M tensors each of size (N_i, self.n_class) for confidence scores for each object \n",
    "                       within each image within the batch\n",
    "                       \n",
    "        :return img_idx: 1-D tensor with size being the total number of objects in the image batch, each \n",
    "                         entry tells which image the object belongs to\n",
    "        :return boxes: 2-D tensor with size (n_total_objects_in_batch, 4)\n",
    "        :return labels: 1-D tensor with size (n_total_objects_in_batch)\n",
    "        :return scores: 1-D tensor with size (n_total_objects_in_batch)\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize a list to keep track of the image corresponding to entries in the list\n",
    "        img_idx, n_images = list(), len(labels)\n",
    "        for idx in range(n_images):\n",
    "            n_objects_in_img = boxes[idx].size(0)\n",
    "            img_idx.extend([idx] * n_objects_in_img)        \n",
    "        img_idx = torch.LongTensor(img_idx).to(device)\n",
    "        boxes   = torch.cat(boxes, dim=0)\n",
    "        labels  = torch.cat(labels, dim=0)\n",
    "        assert img_idx.size(0) == boxes.size(0) == labels.size(0), \"tensor size mismatch\"\n",
    "        if scores is not None:\n",
    "            scores = torch.cat(scores, dim=0)\n",
    "        return {'img'   : img_idx, \n",
    "                'boxes' : boxes, \n",
    "                'labels': labels, \n",
    "                'scores': scores}\n",
    "        \n",
    "        \n",
    "    def class_specific_mAP(truths, preds, category):\n",
    "        \"\"\"\n",
    "        :param truths: dictionary containing ground truth information with keys 'img', 'boxes', 'labels', 'scores'\n",
    "        :param preds:  dictionary containing predicted information with keys 'img', 'boxes', 'labels', 'scores'\n",
    "        :param category: integer representing the category of interest\n",
    "        \n",
    "        : return mAP_class: mean average precision of detections related to category\n",
    "        \"\"\"\n",
    "        # get predictions related to this category\n",
    "        pred_labels = preds['labels']\n",
    "        pred_class_images = preds['img'][pred_labels==category]\n",
    "        pred_class_boxes  = preds['boxes'][pred_labels==category]\n",
    "        pred_class_scores = preds['scores'][pred_labels==category]\n",
    "        n_detections      = pred_class_boxes.size(0)\n",
    "        # mAP is simply 0 if there's nothing detected to be in this class\n",
    "        if n_detections == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # get ground truths related to this category\n",
    "        true_labels = truths['labels']\n",
    "        true_class_images = truths['img'][true_labels==category]\n",
    "        true_class_boxes  = truths['boxes'][true_labels==category]\n",
    "\n",
    "        # re-order scores/images/boxes by descending confidence score\n",
    "        pred_class_scores, sort_idx = torch.sort(pred_class_scores, dim=0, descending=True)\n",
    "        pred_class_images = pred_class_images[sort_idx]\n",
    "        pred_class_boxes  = pred_class_boxes[sort_idx]\n",
    "        \n",
    "        # initialize tensors to keep track of:\n",
    "        # a) which true objects with this class have been 'detected'\n",
    "        true_class_boxes_detected = torch.zeros((true_class_boxes.size(0)), dtype=torch.uint8).to(device)\n",
    "        # b) which detected boxes are true positives\n",
    "        tp = torch.zeros((n_detections), dtype=torch.float).to(device)\n",
    "        # c) which detected boxes are flase positives\n",
    "        fp = torch.zeros((n_detections), dtype=torch.float).to(device)\n",
    "        \n",
    "        # iterate through each detection & check whether it is true-positive or false-positive\n",
    "        for d in range(n_detections):\n",
    "            # get the image this detection is made on + bounding box & score associated with this detection\n",
    "            this_img   = pred_class_images[d]\n",
    "            this_box   = pred_class_boxes[d].unsqueeze(0)\n",
    "            this_score = pred_class_scores[d].unsqueeze(0)\n",
    "            # get ground truth boxes for this image\n",
    "            true_boxes = true_class_boxes[true_class_images==this_img]\n",
    "            # if there are no boxes in this image matching this category, then mark as false-positive\n",
    "            if true_boxes.size(0) == 0:\n",
    "                fp[d] = 1\n",
    "                continue\n",
    "                \n",
    "            # compute Jaccard overlaps; if there are significant level of overlap regions between the \n",
    "            # current (single) detected bounding box and ground truth boxes (multiple), then it is a \n",
    "            # true-positive; false-positives otherwise\n",
    "            overlaps = find_jaccard_overlap(this_box, true_boxes) # (1, n_true_objects_in_img)\n",
    "            max_overlap, idx = torch.max(overlaps.squeenze(0), dim=0)\n",
    "            # get the original_idx position of this object within the true_class_boxes_detected tensor\n",
    "            # this is used to check whether this object has already been detected prior\n",
    "            origin_idx = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images==this_img][idx]            \n",
    "            # if max overlap is greater than 0.5 threshold, this prediction has detected this object\n",
    "            if max_overlap.item() > 0.5:\n",
    "                # check whether this object has been detected before\n",
    "                if true_class_boxes_detected[origin_idx] == 0:\n",
    "                    tp[d] = 1\n",
    "                else:\n",
    "                    fp[d] = 1\n",
    "        \n",
    "        # consolidate how many true-positive & true-positive detections there were\n",
    "        cumsum_tp = torch.cumsum(tp, dim=0)  # (n_class_detections) cumulative sums\n",
    "        cumsum_fp = torch.cumsum(fp, dim=0)  # (n_class_detections) cumulative sums\n",
    "        cumsum_precision = cumsum_tp / (cumsum_tp + cumsum_fp + 1e-10)\n",
    "        cumsum_recall    = cumsum_tp / true_class_boxes.size(0)  # note: we ignored difficulties\n",
    "        \n",
    "        # create thresholds between [0,1] with 0.1 increments\n",
    "        recall_thresholds = torch.arange(start=0, end=1.1, step=0.1).tolist()\n",
    "        precisions        = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)\n",
    "        for i, t in enumerate(recall_thresholds):\n",
    "            recalls_above_t = cumsum_recall >= t\n",
    "            if recalls_above_t.any():\n",
    "                precisions[i] = cumsum_precision[recalls_above_t].max()\n",
    "            else:\n",
    "                precisions[i] = 0.\n",
    "        class_mAP = precisions.mean().item()        \n",
    "        return class_mAP\n",
    "                            \n",
    "        \n",
    "    def __call__(self, pred_boxes, pred_labels, pred_scores, true_boxes, true_labels):\n",
    "        \"\"\"\n",
    "        Takes in both prediction & ground truth labeling for both bounding boxes and object class labels\n",
    "        to compute the mean average precision (mAP). This function operates on a batch of images at a \n",
    "        time, and because each image contain different number of objects within, each input should be \n",
    "        provided as a list of tensors (where each entry of the list is for one particular image). For \n",
    "        example: \n",
    "        \n",
    "        :param pred_boxes: predicted bounding boxes for each image, list of M tensors each of size (N, 4)\n",
    "        :param pred_labels: predicted class label for each image, list of M tensors each of size (N, self.n_class)\n",
    "        :param pred_scores: predicted class score for each image, list of M tensors each of size (N, self.n_class)\n",
    "        :param true_boxes: ground truths bounding boxes, list of M tensors each of size (N, 4)\n",
    "        :param true_labels: ground truths class label for each iamge, list of M tensors each of size (N, self.n_class)\n",
    "        \n",
    "        :return: list of average precisions for all classes, mean average precision (mAP)\n",
    "        \"\"\"\n",
    "        # check length of list of each input is consistent\n",
    "        assert len(pred_boxes) == len(pred_labels) == len(pred_scores) == len(true_boxes) == len(true_labels),\\\n",
    "        \"input tensor length mismatch\"\n",
    "        \n",
    "        # we want to concatenate the list of tensors together within each list, to do that, we first need \n",
    "        # to track which object belongs to which image\n",
    "        truths = concat_batch_tensors(true_boxes, true_labels)\n",
    "        \n",
    "        # because the number of predicted objects may not necessarily match the actual number of objects, we \n",
    "        # also need to do the same for predicted tensors separately\n",
    "        preds  = concat_batch_tensors(pred_boxes, pred_labels, pred_scores)\n",
    "        \n",
    "        # iterate over each category to compute the average precision of the detections for that category \n",
    "        avg_precisions = torch.zeros((self.n_classes - 1), dtype=torch.float)\n",
    "        AP = {}\n",
    "        for c in range(1, self.n_classes):\n",
    "            avg_precisions[c-1] = class_specific_mAP(truths, preds, c)\n",
    "            AP[c] = avg_precisions[c-1]\n",
    "        # further compute the mean over the average precisions over all categories\n",
    "        mAP = avg_precisions.mean().item()\n",
    "        \n",
    "        return mAP, AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_metric = mAP(n_classes=len(ds.id2cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_boxes  = batch['boxes']\n",
    "true_labels = batch['cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([40, 40, 40, 46, 46, 46, 46, 72, 40, 40, 46, 40, 62, 40, 40, 40, 40]),\n",
       " tensor([ 1, 34]),\n",
       " tensor([ 1,  1,  1,  1,  1, 31, 31, 31, 31]),\n",
       " tensor([59, 59, 63, 58, 58, 58, 74, 74, 76, 57, 57, 57, 57, 61, 74, 74, 74, 74,\n",
       "         74, 74, 76, 74, 74, 74, 57, 58]),\n",
       " tensor([16, 57, 66, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 78, 66, 74]),\n",
       " tensor([28,  1,  1,  1, 40, 40, 40]),\n",
       " tensor([ 1,  1,  1, 14, 34, 14, 30, 14]),\n",
       " tensor([ 3, 13,  1,  3])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes  = true_boxes\n",
    "pred_labels = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
