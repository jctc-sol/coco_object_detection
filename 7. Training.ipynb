{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 11 22:16:51 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro M4000        On   | 00000000:00:05.0 Off |                  N/A |\r\n",
      "| 46%   31C    P8    11W / 120W |      1MiB /  8126MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Sun Apr 11 22:16:51 2021\n",
      "Driver Version                            : 450.36.06\n",
      "CUDA Version                              : 11.0\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:00:05.0\n",
      "    Power Readings\n",
      "        Power Management                  : Supported\n",
      "        Power Draw                        : 11.74 W\n",
      "        Power Limit                       : 120.00 W\n",
      "        Default Power Limit               : 120.00 W\n",
      "        Enforced Power Limit              : 120.00 W\n",
      "        Min Power Limit                   : 10.00 W\n",
      "        Max Power Limit                   : 120.00 W\n",
      "    Power Samples\n",
      "        Duration                          : 81.01 sec\n",
      "        Number of Samples                 : 119\n",
      "        Max                               : 44.89 W\n",
      "        Min                               : 11.55 W\n",
      "        Avg                               : 13.22 W\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -q -d POWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from model   import *\n",
    "from metric  import *\n",
    "from loss    import *\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Data transformations, dataset, & dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=5.87s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# define the sequence of transformations to apply to each image sample \n",
    "img_sz = 300\n",
    "\n",
    "# training transforms with data augmentations\n",
    "train_tfms = transforms.Compose(\n",
    "    [PhotometricDistort(1.),\n",
    "     Flip(0.5),\n",
    "     ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "     Zoomout(0.5, max_scale=2.5),\n",
    "     Normalize(), \n",
    "     Resize((img_sz, img_sz))]\n",
    ")\n",
    "\n",
    "# validation transforms without data augmentations\n",
    "tfms = transforms.Compose(\n",
    "    [ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "     Normalize(), \n",
    "     Resize((img_sz, img_sz))]\n",
    ")\n",
    "\n",
    "# instantiate the dataset object\n",
    "ds_train = CocoDataset(data_dir='/datasets/coco', dataset='train2014', anno_type='instances', transforms=train_tfms)\n",
    "ds_valid = CocoDataset(data_dir='/datasets/coco', dataset='val2014', anno_type='instances', transforms=tfms)\n",
    "\n",
    "# create dataloader\n",
    "BS = 16\n",
    "dl_train = DataLoader(ds_train, batch_size=BS, shuffle=True,\n",
    "                      collate_fn=partial(ds_train.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape\n",
    "\n",
    "dl_valid = DataLoader(ds_valid, batch_size=BS, shuffle=True,\n",
    "                      collate_fn=partial(ds_valid.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the SSD model\n",
    "ssd = SSD300(len(ds_train.id2cat), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-loss criteria\n",
    "criterion = MultiBoxLoss(300, ssd.prior_boxes, threshold=0.5, neg_pos_ratio=3, alpha=1., device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = mAP(ssd.n_classes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        self.reset()\n",
    "        self.name = name\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.min = 0\n",
    "        self.max = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def __call__(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt\n",
    "        if (val < self.min): self.min = val\n",
    "        if (val > self.max): self.max = val\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return f'{self.name}_tracker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \"\"\"\n",
    "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
    "    :param optimizer: optimizer with the gradients to be clipped\n",
    "    :param grad_clip: clip value\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp():\n",
    "    \n",
    "    def __init__(self, train_dl, eval_dl, model, criterion, metric, \n",
    "                 name=None, desc=None, display_every_n_batches=None, device=None):\n",
    "        if device is None:\n",
    "            self.device = \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        # meta fields\n",
    "        self.epoch = 0\n",
    "        self.exp_id = f\"exp_{uuid.uuid4()}\"\n",
    "        self.name = name if name is not None else \"model\"\n",
    "        self.desc = desc if desc is not None else \"\"\n",
    "        # dataloader\n",
    "        self.train_dl = train_dl\n",
    "        self.eval_dl  = eval_dl\n",
    "        # model, criterion; put to appropriate device\n",
    "        self.m = model.to(self.device)\n",
    "        self.criterion = criterion.to(self.device)\n",
    "        self.display_every_n_batches = display_every_n_batches\n",
    "        self.metric = metric\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.name} expID: {self.exp_id} @ epoch: {self.epoch}\\n{self.desc}'\n",
    "        \n",
    "        \n",
    "    def setup(self):\n",
    "        # create directory to hold experiment artifacts\n",
    "        import os\n",
    "        os.makedirs(f'./{self.exp_id}/checkpoints', exist_ok=True)        \n",
    "\n",
    "        # trackers to keep track of time\n",
    "        self.epoch_time = Tracker('epoch_time')\n",
    "        self.batch_time = Tracker('batch_time')\n",
    "\n",
    "        # tracker for loss & metric\n",
    "        self.loss_tracker = Tracker('loss')\n",
    "        self.metric_tracker = Tracker('mAP')\n",
    "                \n",
    "        # gather model params to optimize\n",
    "        self.weights, self.biases = list(), list()\n",
    "        for param_name, param in self.m.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param_name.endswith('.bias'):\n",
    "                    self.biases.append(param)\n",
    "                else:\n",
    "                    self.weights.append(param)\n",
    "        \n",
    "                 \n",
    "    def save_checkpoint(self):\n",
    "        state = {'exp_id': self.exp_id,\n",
    "                 'epoch': self.epoch,\n",
    "                 'model': self.m,\n",
    "                 'optimizer': self.optimizer}\n",
    "        filename = f'{self.exp_id}/checkpoints/{self.name}_{self.epoch}epoch.pth.tar'\n",
    "        torch.save(state, filename)\n",
    "\n",
    "        \n",
    "    def load_checkpoint(self, path):\n",
    "        chkpt = torch.load(path)\n",
    "#         self.exp_id = chkpt['exp_id']\n",
    "        self.epoch = chkpt['epoch'] + 1\n",
    "        self.m = chkpt['model']\n",
    "        self.optimizer = chkpt['optimizer']\n",
    "        print(f\"checkpoint loaded; resume training from epoch {self.epoch}\")\n",
    "        self.setup()\n",
    "\n",
    "                 \n",
    "    def train_one_epoch(self, epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"Epoch {epoch} started at {epoch_start_time}\")\n",
    "        self.bs = self.train_dl.batch_size\n",
    "        \n",
    "        # iterate over batches\n",
    "        self.m.train()\n",
    "        for i, batch in enumerate(self.train_dl):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # get ground truth information\n",
    "            images = batch['images'].to(self.device)\n",
    "            boxes  = [b.to(self.device) for b in batch['boxes']]\n",
    "            labels = [c.to(self.device) for c in batch['cats']]\n",
    "            \n",
    "            # forward pass to model & track time taken\n",
    "            pred_boxes, pred_scores = self.m(images)\n",
    "            \n",
    "            # compute loss & backprop\n",
    "            loss = self.criterion(pred_boxes, pred_scores, boxes, labels)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # clip gradient\n",
    "            if self.gradient_clip is not None:\n",
    "                clip_gradient(self.optimizer, self.gradient_clip)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # update trackers\n",
    "            self.loss_tracker(loss.item()/1e3, self.bs)\n",
    "\n",
    "            # time batch time\n",
    "            self.batch_time(time.time() - batch_start_time)\n",
    "            \n",
    "            # Print status\n",
    "            if self.display_every_n_batches is not None:\n",
    "                if i % self.display_every_n_batches == 0:\n",
    "                    # compute mAP\n",
    "                    detected_boxes, detected_labels, detected_scores = self.m.detect_objects(pred_boxes, pred_scores, 0.5, 0.5, 10)\n",
    "                    mean_AP, aps = self.metric(detected_boxes, detected_labels, detected_scores, boxes, labels)\n",
    "                    self.metric_tracker(mean_AP)\n",
    "                    # display progress\n",
    "                    print(f\"Epoch: [{epoch}][{i}/{len(self.train_dl)}]\\t\"\n",
    "                          f\"Avg.time: {self.batch_time.avg:.1f}\\t\"\n",
    "                          f\"Loss: {self.loss_tracker.val:.3f} (Avg: {self.loss_tracker.avg:.3f})\\t\"\n",
    "                          f\"mAP: {self.metric_tracker.val:.3f} (Avg: {self.metric_tracker.avg:.3f})\")\n",
    "        \n",
    "        # free some memory since their histories may be stored\n",
    "        del pred_boxes, pred_scores, images, boxes, labels\n",
    "        # display duration & update epoch\n",
    "        print(f\"Epoch {epoch} completed; duration {(time.time() - epoch_start_time)/60.} minutes\")\n",
    "\n",
    "    \n",
    "    def eval(self, dl, name=None):\n",
    "        if not name: name = 'eval'\n",
    "        eval_start_time = time.time()\n",
    "        \n",
    "        # set model to eval mode\n",
    "        self.m.eval()\n",
    "        with torch.no_grad():\n",
    "            # iterate over dataloader\n",
    "            for i, batch in enumerate(dl): \n",
    "                # get ground truth information\n",
    "                images = batch['images'].to(self.device)\n",
    "                boxes  = [b.to(self.device) for b in batch['boxes']]\n",
    "                labels = [c.to(self.device) for c in batch['cats']]\n",
    "                # get prediction\n",
    "                pred_boxes, pred_scores = self.m(images)\n",
    "                detected_boxes, detected_labels, detected_scores = ssd.detect_objects(pred_boxes, pred_scores, 0.5, 0.5, 10)\n",
    "                # compute metric\n",
    "                mean_AP, aps = self.metric(detected_boxes, detected_labels, detected_scores, boxes, labels)\n",
    "                self.metric_tracker(mean_AP)\n",
    "        \n",
    "        # display metric\n",
    "        print(f\"Min mAP: {self.metric_tracker.min}\\t\"\n",
    "              f\"Max mAP: {self.metric_tracker.max}\\t\"\n",
    "              f\"Avg mAP: {self.metric-tracker.avg}\")\n",
    "        print(f\"Evaluation took {time.time() - eval_start_time} seconds\")\n",
    "\n",
    "                 \n",
    "    def train(self, n_epochs, optimizer, lr, gradient_clip=None, eval_every_n_epochs=10, save_every_n_epoch=1):\n",
    "        # setup experiment\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.save_every_n_epoch = save_every_n_epoch\n",
    "        \n",
    "        # iterate over epochs\n",
    "        for n in range(n_epochs):\n",
    "            \n",
    "            # evaluate on validation dataset every n epochs\n",
    "            if (n > 0) and (n % eval_every_n_epochs == 0):\n",
    "                print('here')\n",
    "                self.eval(self.eval_dl)\n",
    "                \n",
    "            # run one epoch training\n",
    "            self.train_one_epoch(n)\n",
    "            \n",
    "            # save every n epoch\n",
    "            if n % self.save_every_n_epoch == 0:\n",
    "                self.save_checkpoint()\n",
    "            \n",
    "            self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Experiment & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather ingredients for experiment\n",
    "ingredients = {\n",
    "    'train_dl' : dl_train,\n",
    "    'eval_dl'  : dl_valid,\n",
    "    'model'    : ssd,\n",
    "    'criterion': criterion,\n",
    "    'metric'   : metric,\n",
    "    'name'     : 'ssd',\n",
    "    'desc'     : 'training ssd on coco2014 dataset',\n",
    "    'display_every_n_batches' : 50,\n",
    "    'device'   : device\n",
    "}\n",
    "# init exerpiment & setup\n",
    "exp = Exp(**ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train from scratch\n",
    "exp.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssd expID: exp_ab068a56-6bb2-4501-8f1c-d8dc41c0c211 @ epoch: 0\n",
       "training ssd on coco2014 dataset"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from checkpoint\n",
    "chkpt_path = 'exp_95788307-930e-4c0d-9c93-f95f49872adb/checkpoints/ssd_0epoch.pth.tar'\n",
    "exp.load_checkpoint(chkpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define & init optimizer\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.SGD(params=[{'params': exp.biases, 'lr': 2 * lr}, {'params': exp.weights}], # update biases at 2x LR over weights\n",
    "                            lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for 1 epoch\n",
    "exp.train(1, optimizer, lr, gradient_clip=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for x epochs\n",
    "exp.train(1, optimizer, lr, gradient_clip=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.epoch = 2\n",
    "exp.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
