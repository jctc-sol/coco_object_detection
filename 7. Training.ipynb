{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from model   import *\n",
    "from metric  import *\n",
    "from loss    import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Data transformations, dataset, & dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# define the sequence of transformations to apply to each image sample \n",
    "img_sz = 300\n",
    "basic_tfs = [PhotometricDistort(1.),\n",
    "             Flip(0.5),\n",
    "             ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "             Zoomout(0.5, max_scale=2.5),\n",
    "             Normalize(), \n",
    "             Resize((img_sz, img_sz))]\n",
    "tfms = transforms.Compose(basic_tfs)\n",
    "\n",
    "# instantiate the dataset object\n",
    "ds = CocoDataset(data_dir='./', dataset='val2017', anno_type='instances', transforms=tfms)\n",
    "\n",
    "# create dataloader\n",
    "BS = 8\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True, \n",
    "                collate_fn=partial(ds.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the SSD model\n",
    "ssd = SSD300(len(ds.id2cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-loss criteria\n",
    "criterion = MultiBoxLoss(300, ssd.prior_boxes, threshold=0.5, neg_pos_ratio=3, alpha=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = mAP(n_classes=ssd.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        self.reset()\n",
    "        self.name = name\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def __call__(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \"\"\"\n",
    "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
    "    :param optimizer: optimizer with the gradients to be clipped\n",
    "    :param grad_clip: clip value\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp():\n",
    "    \n",
    "    def __init__(self, dataloader, model, criterion, name=None, desc=None, verbose=None):\n",
    "        global device \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # meta fields\n",
    "        self.name = name if name is not None else \"model\"\n",
    "        self.desc = desc if desc is not None else \"...\"\n",
    "        # dataloader, model, criterion; put to appropriate device\n",
    "        self.dl = dataloader\n",
    "        self.bs = dataloader.batch_size\n",
    "        self.m = model.to(device)\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        \n",
    "    def setup(self):\n",
    "        # init training process params\n",
    "        self.epoch = 0        \n",
    "        # create directory to hold experiment artifacts\n",
    "        import os\n",
    "        self.exp_id = uuid.uuid4()\n",
    "        os.makedirs(f'./{self.exp_id}/checkpoints', exist_ok=True)        \n",
    "\n",
    "        # create running trackers to keep track of time, loss, metrics\n",
    "        self.data_time = Tracker('data_time')   \n",
    "        self.fwd_time = Tracker('fwd_time')\n",
    "        self.criterion_time = Tracker('criterion_time')\n",
    "        self.bkwd_time = Tracker('bkwd_time')\n",
    "        self.batch_time = Tracker('batch_time')\n",
    "        self.loss = Tracker('loss')\n",
    "    \n",
    "        # gather model params to optimize\n",
    "        self.weights, self.biases = list(), list()\n",
    "        for param_name, param in self.m.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param_name.endswith('.bias'):\n",
    "                    self.biases.append(param)\n",
    "                else:\n",
    "                    self.weights.append(param)\n",
    "        \n",
    "                 \n",
    "    def save_checkpoint(epoch, model, optimizer):\n",
    "        state = {'epoch': epoch,\n",
    "                 'model': model,\n",
    "                 'optimizer': optimizer}\n",
    "        filename = f'{self.exp_id}/checkpoints/{self.name}_{epoch}epoch.pth.tar'\n",
    "        torch.save(state, filename)\n",
    "\n",
    "        \n",
    "    def load_checkpoint(self, path):        \n",
    "        chkpt = torch.load(path)\n",
    "        self.epoch = chkpt['epoch'] + 1; print(f\"checkpoint loaded; resume training from epoch {self.epoch}\")\n",
    "        self.m = chkpt['model']\n",
    "        self.optimizer = chkpt['optimizer']\n",
    "    \n",
    "                 \n",
    "    def train_one_epoch(self, epoch):\n",
    "        start_time = time.time()\n",
    "        # iterate over batches\n",
    "        for i, batch in enumerate(self.dl):            \n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # update data time to record how long it takes to load one batch of data\n",
    "            self.data_time(time.time() - start_time)\n",
    "            # get ground truth information\n",
    "            images = batch['images'].to(device)\n",
    "            boxes  = [b.to(device) for b in batch['boxes']]\n",
    "            labels = [c.to(device) for c in batch['cats']]\n",
    "            \n",
    "            # forward pass to model & track time taken\n",
    "            t = time.time()\n",
    "            pred_boxes, pred_scores = self.m(images)\n",
    "            self.fwd_time(time.time() - t)\n",
    "            \n",
    "            # compute loss & track time taken\n",
    "            t = time.time()\n",
    "            loss = self.criterion(pred_boxes, pred_scores, boxes, labels)\n",
    "            self.criterion_time(time.time() - t)\n",
    "            \n",
    "            # update params\n",
    "            t = time.time()\n",
    "            self.optimizer.zero_grad()\n",
    "            # back-prop\n",
    "            loss.backward()\n",
    "            # clip gradient\n",
    "            if self.gradient_clip is not None:\n",
    "                clip_gradient(self.optimizer, self.gradient_clip)\n",
    "            # update trainable params\n",
    "            self.optimizer.step()\n",
    "            self.bkwd_time(time.time() - t)\n",
    "            \n",
    "            # update trackers\n",
    "            self.loss(loss.item(), images.size(0))\n",
    "            self.batch_time(time.time() - batch_start_time)\n",
    "\n",
    "            # Print status\n",
    "            if self.verbose is not None:\n",
    "                if i % self.verbose == 0:\n",
    "                    print(f\"Epoch: [{epoch}][{i}/{len(self.dl)}]\\t\"\n",
    "                          f\"Batch time: {self.batch_time.val:.3f} ({self.batch_time.avg:.3f})\\t\"\n",
    "                          f\"Loss: {self.loss.val:.4f} ({self.loss.avg:.4f})\"\n",
    "                         )\n",
    "        # free some memory since their histories may be stored\n",
    "        del predicted_locs, predicted_scores, images, boxes, labels  \n",
    "\n",
    "                 \n",
    "    def train(self, n_epochs, optimizer, lr, gradient_clip=None, eval_every_n_epoch=1, save_every_n_epoch=1):\n",
    "        # setup experiment\n",
    "        self.setup()\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.save_every_n_epoch = save_every_n_epoch\n",
    "        \n",
    "        # iterate over epochs\n",
    "        for n in range(n_epochs):\n",
    "            # run one epoch training\n",
    "            self.train_one_epoch(n)\n",
    "            # evaluate every n epoch\n",
    "            pass\n",
    "            # save every n epoch\n",
    "            if n % self.save_every_n_epoch == 0:\n",
    "                self.save_checkpoint(n, self.m, self.optimizer)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Experiment & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init experiment object\n",
    "exp = Exp(dl, ssd, criterion, \"SSD\", verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define & init optimizer\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.SGD(params=[{'params': exp.biases, 'lr': 2 * lr}, {'params': exp.weights}], # update biases at 2x LR over weights\n",
    "                            lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for x epochs\n",
    "exp.train(5, optimizer, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
