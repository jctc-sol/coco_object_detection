{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from model   import SSD300\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.67s)\n",
      "creating index...\n",
      "index created!\n",
      "image batch tensor shape: torch.Size([8, 3, 300, 300])\n",
      "bounding box location prediction shape: torch.Size([8, 8732, 4])\n",
      "object class prediction shape: torch.Size([8, 8732, 81])\n"
     ]
    }
   ],
   "source": [
    "# define the sequence of transformations to apply to each image sample \n",
    "basic_tfs = [PhotometricDistort(1.),\n",
    "             Flip(0.5),\n",
    "             ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "             Zoomout(0.5, max_scale=2.5),\n",
    "             Normalize(), \n",
    "             Resize((300,300))]\n",
    "tfms = transforms.Compose(basic_tfs)\n",
    "\n",
    "# instantiate the dataset object\n",
    "ds = CocoDataset(data_dir='./', dataset='val2017', anno_type='instances', transforms=tfms)\n",
    "\n",
    "# create dataloader\n",
    "BS = 8\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True, \n",
    "                collate_fn=partial(ds.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape\n",
    "\n",
    "# create model object\n",
    "ssd = SSD300(len(ds.id2cat))\n",
    "\n",
    "# test forward pass for one batch\n",
    "for batch in dl:\n",
    "    image_batch = batch['images']\n",
    "    print(f\"image batch tensor shape: {image_batch.size()}\")\n",
    "    # forward pass through SSD300\n",
    "    pred_boxes, pred_scores = ssd(image_batch)\n",
    "    print(f\"bounding box location prediction shape: {pred_boxes.size()}\")\n",
    "    print(f\"object class prediction shape: {pred_scores.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate Systems\n",
    "\n",
    "The native coordinate system for the COCO dataset for the bounding boxes are expressed in terms of $(x, y, w, h)$, where $(x, y)$ coordinates are measured from the top left image corner $(0, 0)$. We introduce three sets of coordinate systems that are utilized through the model prediction / optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO coordinates to center coordinates\n",
    "This transformation ecodes/decodes the original COCO bounding box coordinates $(x, y, w, h)$ (where $(x, y)$ represent the top-left corner of bounding box) to center coordinates $(x_c, y_c, w_c, h_c)$ where $(x_c, y_c)$ represent the center of the bounding box, furthermore, both $(x_c, y_c)$ and $(w_c, h_c)$ are normalized with respect to the original size of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coco2CenterCoord():\n",
    "    \"\"\"\n",
    "    Encodes/Decodes original COCO bounding box coordinates (x, y, w, h) where (x, y)\n",
    "    represent the top-left corner of bounding box (in image coordinate frame) to center \n",
    "    coordinates (x_c, y_c, w_c, h_c) where (x_c, y_c) represent the center of the bounding box, \n",
    "    furthermore, both (x_c, y_c) and (w_c, h_c) are normalized with respect to the original \n",
    "    size of image\n",
    "    \"\"\"       \n",
    "    def __init__(self, w, h):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        \n",
    "    def encode(self, boxes):\n",
    "        \"\"\"\n",
    "        boxes: bounding boxes tensor with coordinates in original COCO (x, y, w, h) format\n",
    "        \"\"\"\n",
    "        x_c = (boxes[:,0] + boxes[:,2]/2.0)/self.w\n",
    "        y_c = (boxes[:,1] + boxes[:,3]/2.0)/self.h\n",
    "        w_c = boxes[:,2]/self.w\n",
    "        h_c = boxes[:,3]/self.h\n",
    "        coords = [x_c, y_c, w_c, h_c]        \n",
    "        return torch.cat([c.unsqueeze(-1) for c in coords], dim=-1)\n",
    "    \n",
    "    def decode(self, boxes_c):\n",
    "        \"\"\"\n",
    "        boxes_c: bounding boxes tensor with coordinates in center coordinates (x_c, y_c, w_c, h_c) format\n",
    "        \"\"\"\n",
    "        x = (boxes_c[:,0] - boxes_c[:,2]/2.0) * self.w\n",
    "        y = (boxes_c[:,1] - boxes_c[:,3]/2.0) * self.h\n",
    "        width  = boxes_c[:,2] * self.w\n",
    "        height = boxes_c[:,3] * self.h\n",
    "        coords = [x, y, width, height]        \n",
    "        return torch.cat([c.unsqueeze(-1) for c in coords], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single dataset sample\n",
    "sample = ds[0]\n",
    "_, h, w = sample['image'].size()\n",
    "boxes_before = sample['boxes']\n",
    "\n",
    "# instantiate transform\n",
    "ccoord = Coco2CenterCoord(w, h)\n",
    "\n",
    "# transform box coordinates\n",
    "boxes_after = ccoord.encode(boxes_before)\n",
    "\n",
    "# inverse transform\n",
    "boxes_inverse = ccoord.decode(boxes_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box cooridnates before transformation:\n",
      " tensor([[111.0844, 100.3592,  11.5781,  48.9437],\n",
      "        [  3.2953, 118.1408,  69.9938,  66.8099],\n",
      "        [261.1922, 147.3169,  38.1328,  55.4437]])\n",
      "\n",
      "box coordinates after transformation:\n",
      " tensor([[0.3896, 0.4161, 0.0386, 0.1631],\n",
      "        [0.1276, 0.5052, 0.2333, 0.2227],\n",
      "        [0.9342, 0.5835, 0.1271, 0.1848]])\n",
      "\n",
      "box coordinates apply inverse transformation:\n",
      " tensor([[111.0844, 100.3592,  11.5781,  48.9437],\n",
      "        [  3.2953, 118.1408,  69.9938,  66.8099],\n",
      "        [261.1922, 147.3169,  38.1328,  55.4437]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"box cooridnates before transformation:\\n\", boxes_before[:3,:]);\n",
    "print(f\"\\nbox coordinates after transformation:\\n\", boxes_after[:3,:]);\n",
    "print(f\"\\nbox coordinates apply inverse transformation:\\n\", boxes_inverse[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center coordinates to Boundary coordinates\n",
    "\n",
    "Encodes/decodes the bounding box center coordinates $(x_{c}, y_{c}, w_{c}, h_{c})$ to/from boundary coordinates $(x_{1}, y_{1}, x_{2}, y_2)$ where $(x_1, y_1)$ specifies the upper-left corner and $(x_2, y_2)$ the lower-right corner of the boundary of bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryCoord():\n",
    "    \"\"\"\n",
    "    Encodes/decodes the bounding box center coordinates (x_c, y_c, w_c, h_c) to/from boundary coordinates \n",
    "    (x_1, y_1, x_2, y_2) where (x_1, y_1) specifies the upper-left corner and (x_2, y_2) the lower-right\n",
    "    corner of the boundary of bounding boxes\n",
    "    \"\"\"        \n",
    "    def encode(self, boxes):\n",
    "        \"\"\"\n",
    "        boxes: bounding boxes tensor in center coordinates (x_c, y_c, w_c, h_c) format\n",
    "        return: bounding boxes tensor in boundary coordinates (x_1, y_1, x_2, y_2) format\n",
    "        \"\"\"\n",
    "        x1 = boxes[:,0] - boxes[:,2]/2.0\n",
    "        y1 = boxes[:,1] - boxes[:,3]/2.0\n",
    "        x2 = boxes[:,0] + boxes[:,2]/2.0\n",
    "        y2 = boxes[:,1] + boxes[:,3]/2.0        \n",
    "        coords = [x1, y1, x2, y2]        \n",
    "        return torch.cat([c.unsqueeze(-1) for c in coords], dim=-1)    \n",
    "        \n",
    "    def decode(self, boxes):\n",
    "        \"\"\"\n",
    "        boxes: bounding boxes tensor in boundary coordinates (x_1, y_1, x_2, y_2) format\n",
    "        return: bounding boxes tensor in center coordinates (x_c, y_c, w_c, h_c) format\n",
    "        \"\"\"\n",
    "        w_c = boxes[:,2] - boxes[:,0]\n",
    "        h_c = boxes[:,3] - boxes[:,1]\n",
    "        x_c = boxes[:,0] + w_c/2.0\n",
    "        y_c = boxes[:,1] + h_c/2.0\n",
    "        coords = [x_c, y_c, w_c, h_c]\n",
    "        return torch.cat([c.unsqueeze(-1) for c in coords], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single dataset sample\n",
    "sample = ds[0]\n",
    "boxes = sample['boxes']\n",
    "_, h, w = sample['image'].size()\n",
    "\n",
    "# instantiate transforms\n",
    "ccoord = Coco2CenterCoord(w, h)\n",
    "bcoord = BoundaryCoord()\n",
    "\n",
    "# transform bounding boxes from Coco-coordinates to center-coordinates\n",
    "boxes_center = ccoord.encode(boxes)\n",
    "# transform bounding boxes from center-coordinates to boundary-coordinates\n",
    "boxes_boundary = bcoord.encode(boxes_center)\n",
    "# transform from boundary-coordinates back to center-coordinates\n",
    "boxes_center_  = bcoord.decode(boxes_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coco coordinates:\n",
      "tensor([[195.2894, 101.5071,   8.7176,  36.8375],\n",
      "        [232.4647, 114.8905,  52.7012,  50.2845],\n",
      "        [ 62.2729, 136.8498,  28.7118,  41.7297]])\n",
      "\n",
      "Coco coordinates -> center coordinates:\n",
      "tensor([[0.6655, 0.3998, 0.0291, 0.1228],\n",
      "        [0.8627, 0.4668, 0.1757, 0.1676],\n",
      "        [0.2554, 0.5257, 0.0957, 0.1391]])\n",
      "\n",
      "center coordinates -> boundary coordinates:\n",
      "tensor([[0.6510, 0.3384, 0.6800, 0.4611],\n",
      "        [0.7749, 0.3830, 0.9506, 0.5506],\n",
      "        [0.2076, 0.4562, 0.3033, 0.5953]])\n",
      "\n",
      "boundary coordinates -> center coordinates:\n",
      "tensor([[0.6655, 0.3998, 0.0291, 0.1228],\n",
      "        [0.8627, 0.4668, 0.1757, 0.1676],\n",
      "        [0.2554, 0.5257, 0.0957, 0.1391]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coco coordinates:\\n{boxes[:3]}\")\n",
    "print(f\"\\nCoco coordinates -> center coordinates:\\n{boxes_center[:3]}\")\n",
    "print(f\"\\ncenter coordinates -> boundary coordinates:\\n{boxes_boundary[:3]}\")\n",
    "print(f\"\\nboundary coordinates -> center coordinates:\\n{boxes_center_[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center coordinates to Prior Box coordinate offsets\n",
    "\n",
    "For the localization aspect of SSD prediction, the model **predicts** the *\"offsets relative to the default box shapes in the cell\"* at each of the feature map grid locations.\n",
    "\n",
    "- For the bounding box center coordinates $(x_c, y_c)$ of $(x_c, y_c, w_c, h_c)$ relative to prior box coordinates $(x_p, y_p, w_p, h_p)$, express offset $({\\Delta}x_c, {\\Delta}y_c) = (\\frac{(x_c - x_p)}{w_p}, \\frac{(y_c - y_p)}{h_p})$; and\n",
    "\n",
    "- For the bounding box shape coordinates $(w_c, h_c)$, express the shape offset $({\\Delta}w_c, {\\Delta}h_c) = (\\log{(\\frac{w_c}{w_p})}, \\log{(\\frac{h_c}{h_p})})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffsetCoord():\n",
    "    \"\"\"\n",
    "    Encodes/decodes the center coordinates (x_c, y_c, w_c, h_c) of bounding boxes relative to the prior \n",
    "    boxes (from SSD, expressed also in center coordinates) in terms of offset coordinates. This offset \n",
    "    coordinates is the form that is output by the SSD locator prediction. The offset coordinates have \n",
    "    the following relation:\n",
    "    (dx, dy) = ((x_c - x_p)/(x_p/10), (y_c - y_p)/(y_p/10)); and \n",
    "    (dw, dh) = (log(w_c/(w_p*5)), log(h_c/(h_p*5)))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def encode(self, cxcy, priors_cxcy):\n",
    "        \"\"\"\n",
    "        cxcy: bounding box in center-coordinate format\n",
    "        prior_cxcy: prior box in center-coordinate format\n",
    "        \"\"\"\n",
    "        dxdy = (cxcy[:,:2] - priors_cxcy[:,:2]) / (priors_cxcy[:,2:] / 10)\n",
    "        dwdh = torch.log(cxcy[:,2:] / priors_cxcy[:,2:]) * 5\n",
    "        return torch.cat([dxdy, dwdh], dim=1)\n",
    "    \n",
    "    \n",
    "    def decode(self, dxdy, priors_cxcy):\n",
    "        \"\"\"\n",
    "        dxdy: bounding boxes in offset-coordinate format wrt SSD's prior bounding boxes\n",
    "        \"\"\"\n",
    "        cxcy = dxdy[:,:2] * priors_cxcy[:,2:] / 10 + priors_cxcy[:,:2]\n",
    "        cwch = torch.exp(dxdy[:,2:] / 5) * priors_cxcy[:,2:]\n",
    "        return torch.cat([cxcy, cwch], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding box in center coordinates:\n",
      "tensor([[0.3896, 0.4161, 0.0386, 0.1631],\n",
      "        [0.1276, 0.5052, 0.2333, 0.2227],\n",
      "        [0.9342, 0.5835, 0.1271, 0.1848]])\n",
      "\n",
      "prior box in center coordinates:\n",
      "tensor([[0.0132, 0.0132, 0.1000, 0.1000],\n",
      "        [0.0132, 0.0132, 0.1414, 0.1414],\n",
      "        [0.0132, 0.0132, 0.1414, 0.0707]])\n",
      "\n",
      "bounding box in offset coordinates:\n",
      "tensor([[37.6420, 40.2945, -4.7604,  2.4474],\n",
      "        [ 8.0952, 34.7893,  2.5032,  2.2704],\n",
      "        [65.1272, 80.6532, -0.5335,  4.8037]])\n",
      "\n",
      "bounding box converted back to center coordinates:\n",
      "tensor([[0.3896, 0.4161, 0.0386, 0.1631],\n",
      "        [0.1276, 0.5052, 0.2333, 0.2227],\n",
      "        [0.9342, 0.5835, 0.1271, 0.1848]])\n"
     ]
    }
   ],
   "source": [
    "# prior bounding boxes already in center coords\n",
    "prior_boxes = ssd.prior_boxes\n",
    "\n",
    "# init offset coord object\n",
    "ocoord = OffsetCoord()\n",
    "\n",
    "# select one bounding box location (already encoded in center coordinate format)\n",
    "bbox = boxes_after[:3,:]\n",
    "# select one prior box location (also encoded in center coordinate format)\n",
    "pbox = prior_boxes[:3,:]\n",
    "\n",
    "# pick one location prediction for demonstration purpose\n",
    "# encode the locational prediction output in offset coordinates, related to SSD prior bounding boxes\n",
    "bbox_offset = ocoord.encode(bbox, pbox)\n",
    "bbox_offset_inv = ocoord.decode(bbox_offset, pbox)\n",
    "\n",
    "print(f\"bounding box in center coordinates:\\n{bbox}\\n\")\n",
    "print(f\"prior box in center coordinates:\\n{pbox}\\n\")\n",
    "print(f\"bounding box in offset coordinates:\\n{bbox_offset}\\n\")\n",
    "print(f\"bounding box converted back to center coordinates:\\n{bbox_offset_inv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
