{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from torchvision.models import vgg16\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from model   import *\n",
    "from metric  import *\n",
    "from math import sqrt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Box Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss funcion for object detection, which is a linear combination of:\n",
    "    a) object localization loss for the predicted bounding box location; and\n",
    "    b) classification loss for the predicted object class\n",
    "    \n",
    "    Code reference: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py#L532\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_sz, pboxes, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
    "        \"\"\"\n",
    "        :param img_sz: input image size into object detection model (assumed to be square)\n",
    "        :param pboxes: prior bounding boxes of object detection model in center coordinates\n",
    "        :param threshold: cutoff threshold on IoU overlap between a pair of true object box and prior bounding box\n",
    "        :param neg_pos_ratio: ratio to be used in hard negative sample minning\n",
    "        :param alpha: relative weighting between localization & classification losses\n",
    "        \"\"\"\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.pboxes = pboxes\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "        # localization/classification losses\n",
    "        self.loc_loss = nn.L1Loss()\n",
    "        self.cls_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "        # coordinate transforms\n",
    "        self.cocoCoord = Coco2CenterCoord(img_sz,img_sz)\n",
    "        self.boundaryCoord = BoundaryCoord()\n",
    "        self.offsetCoord = OffsetCoord()\n",
    "        \n",
    "        \n",
    "    def forward(self, pred_boxes, pred_scores, true_boxes, true_classes):\n",
    "        \"\"\"\n",
    "        Forward pass to compute the loss given predicted bounding boxes and predicted classification scores\n",
    "        from an object detection model. N for batch size below.\n",
    "        :param pred_boxes:  predicted bound boxes from object detection model in offset coordinates form; tensor of dim (N, 8732, 4)\n",
    "        :param pred_scores: predicted classification scores from boject detection model; tensor of dim (N, 8732, n_classes)\n",
    "        :param true_boxes: ground truth label on location of each object in a batch of images, expressed in boundary coordinates; list of N tensors\n",
    "        :param true_classes: grounth truth label on class of each object in a batch of images; list of N tensors\n",
    "        \n",
    "        :return: scalar loss measure\n",
    "        \"\"\"\n",
    "        n_priors  = self.pboxes.size(0)\n",
    "        n_classes = pred_scores.size(-1)\n",
    "        bs = pred_boxes.size(0)\n",
    "        assert n_priors == pred_boxes.size(1) == pred_scores.size(1)\n",
    "        \n",
    "        # init tensors for recording all ground truth objects/labels allocated to each prior bounding boxes\n",
    "        true_locs = torch.zeros_like(pred_boxes, dtype=torch.float).to(device)        # (N, 8732, 4)\n",
    "        true_cls  = torch.zeros((bs, n_priors), dtype=torch.long).to(device)  # (N, 8732)\n",
    "        \n",
    "        # for each image in batch, we want to find the best ground truth object that each prior bounding box \n",
    "        # captures in terms of maximum IoU overlap. More specifically, we want to:\n",
    "        # a) assign an object class to each prior bounding box that reflect the object class each prior box best overlaps with;\n",
    "        #    a cutoff threshold is applied to suppress prior bounding boxes to background if IoU falls below this threhsold\n",
    "        # b) compute how \"off\" each prior bound box location coordinate is relative to the ground truth object it has the best\n",
    "        #    overlap with (i.e. as offset coordinates)\n",
    "        # and populate `true_cls` & `true_locs` so they captured all the class/location-offset assignment for all prior bounding \n",
    "        # boxes for each image in the batch\n",
    "        for i in range(bs):\n",
    "            # get number of ground truth objects in image i\n",
    "            n_objs = true_boxes[i].size(0)\n",
    "            # find overlap of each ground truth objects with each of the prior bounding boxes\n",
    "            overlaps = find_jaccard_overlap(true_boxes[i], self.pboxes)  # (n_objects, 8732)\n",
    "            \n",
    "            # find the best ground truth object overlaping with each prior bounding boxes\n",
    "            obj_overlap_for_each_prior, obj_idx_for_each_prior = overlaps.max(dim=0)  # (8732)\n",
    "            # find the best bounding box overlap with each ground truth objects\n",
    "            _, best_pbox_for_each_obj = overlaps.max(dim=1)  # (n_objects)\n",
    "                        \n",
    "            # ** two potential problem scenarios to mitigate:\n",
    "            # 1) none of the prior bounding boxes have overlap with groundtruth object > 0.5 and therefore the object is taken as background\n",
    "            # Solution: assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n",
    "            obj_idx_for_each_prior[best_pbox_for_each_obj] = torch.LongTensor(range(n_objs)).to(device)\n",
    "            # 2) a groundtruth object is not found as the maximum overlapped object with any of the prior bounding boxes\n",
    "            # Solution: artificially set IoU overlap with the best bounding box to 1 to ensure each object is captured by 1 prior bounding box\n",
    "            obj_overlap_for_each_prior[best_pbox_for_each_obj] = 1.\n",
    "\n",
    "            # get object class label for each prior bounding box\n",
    "            obj_label_for_each_prior = true_classes[i][obj_idx_for_each_prior]\n",
    "            # for those with overlap < threshold, suppress object as background (i.e. class_label=0)\n",
    "            obj_label_for_each_prior[obj_overlap_for_each_prior < self.threshold] = 0\n",
    "            \n",
    "            # add true object class label allocation for each prior bounding box\n",
    "            true_cls[i]  = obj_label_for_each_prior\n",
    "            # add true object locations for each prior bounding box in the form of offset distance of each prior bounding box wrt the \n",
    "            # ground truth object box with the best overlap\n",
    "            \n",
    "            true_locs[i] = self.offsetCoord.encode(true_boxes[i][obj_idx_for_each_prior], self.pboxes)\n",
    "\n",
    "        # create flag for all non-background prior bounding boxes (i.e. class label = 0)\n",
    "        positive_priors = true_cls != 0\n",
    "        # LOCALIZATION LOSS across non-background prior bounding boxes\n",
    "        loc_loss = self.loc_loss(pred_boxes[positive_priors], true_locs[positive_priors])\n",
    "        \n",
    "        # compute classification loss for all prior bounding boxes\n",
    "        cls_loss_all = self.cls_loss(pred_scores.view(-1, n_classes), true_cls.view(-1))\n",
    "        cls_loss_all = cls_loss_all.view(bs, n_priors)  # (N, 8732)\n",
    "        \n",
    "        # POSITIVE PRIOR CLASSIFICATION LOSS\n",
    "        # gather the classification loss for all the positive prior bounding boxes\n",
    "        cls_loss_pos_priors = cls_loss_all[positive_priors].sum()\n",
    "        \n",
    "        # Hard-Negative-Mining (HNM)\n",
    "        # HNM is used in the case where there is a large imbalance between negative vs positive class \n",
    "        # ground truth objects. In the context of object detection this is often the case as the vast \n",
    "        # majority of bounding boxes would capture background (i.e. class = 0). Thus we artificially\n",
    "        # balance out the negative vs positive class ratio by selecting `n` number of negative samples\n",
    "        # with the largest loss (i.e. hardest negative samples) and include those in our loss computation\n",
    "        # along with the positive classes\n",
    "        n_positives = positive_priors.sum(dim=1).sum().float()\n",
    "        n_neg_samples = self.neg_pos_ratio * n_positives\n",
    "        # set positive prior losses to 0 since we've already computed cls_loss_pos_priors\n",
    "        cls_loss_neg = cls_loss_all.clone()\n",
    "        cls_loss_neg[positive_priors] = 0.\n",
    "        # sort losses in decending order\n",
    "        cls_loss_neg, _ = cls_loss_all.sort(dim=1, descending=True)\n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(cls_loss_neg).to(device)  # (N, 8732)        \n",
    "        hard_neg = hardness_ranks < n_neg_samples.unsqueeze(-1) # (N, 8732)\n",
    "        # HNM LOSS\n",
    "        cls_loss_hard_neg = cls_loss_neg[hard_neg].sum()\n",
    "        \n",
    "        # COMBINED CLASSIFICATION LOSS\n",
    "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
    "        cls_loss = (cls_loss_pos_priors + cls_loss_hard_neg) / n_positives\n",
    "        \n",
    "        return loc_loss + self.alpha * cls_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: mini-training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.47s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# define the sequence of transformations to apply to each image sample \n",
    "basic_tfs = [PhotometricDistort(1.),\n",
    "             Flip(0.5),\n",
    "             ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "             Zoomout(0.5, max_scale=2.5),\n",
    "             Normalize(), \n",
    "             Resize((300,300))]\n",
    "tfms = transforms.Compose(basic_tfs)\n",
    "\n",
    "# instantiate the dataset object\n",
    "ds = CocoDataset(data_dir='./', dataset='val2017', anno_type='instances', transforms=tfms)\n",
    "\n",
    "# create dataloader\n",
    "BS = 8\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True, \n",
    "                collate_fn=partial(ds.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape\n",
    "\n",
    "# create the SSD model\n",
    "ssd = SSD300(len(ds.id2cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-loss criteria\n",
    "criterion = MultiBoxLoss(300, ssd.prior_boxes, threshold=0.5, neg_pos_ratio=3, alpha=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward pass for one batch\n",
    "for batch in dl:\n",
    "    image_batch = batch['images']\n",
    "    boxes = batch['boxes']\n",
    "    labels = batch['cats']\n",
    "    \n",
    "    # forward pass through SSD300\n",
    "    pred_boxes, pred_scores = ssd(image_batch)\n",
    "    \n",
    "    # compute loss over image batch\n",
    "    loss = criterion(pred_boxes, pred_scores, boxes, labels)  # scalar\n",
    "    # back-prop\n",
    "    loss.backward()\n",
    "    \n",
    "    # use the predictions to detect objects\n",
    "    detected_boxes, detected_labels, detected_scores = ssd.detect_objects(\n",
    "        pred_boxes, pred_scores, min_score_threshold=0.1, max_overlap_threshold=0.5, top_k=10\n",
    "    )\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
