{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from torchvision.models import vgg16\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from math import sqrt\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "The model architecture for the Single Shot MultiBox Detector (SSD) is shown below (taken from the original SSD paper).\n",
    "![SSD architecture from arXiv:1512.02325v5](./illustrations/SSD_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Base\n",
    "\n",
    "The first starting point is to leverage the pre-trained VGG-16 model (without batch normalizations) for the SSD backbone. All convolution and pooling layers are kept under the \"features\" sequential portion of VGG-16 (i.e. up to `Conv5_3` layer as show in the diagram above). \n",
    "\n",
    "The SSD architecture then converted the fully connected layers `FC6` and `FC7` to convolution layers. In order to get the shapes to align, the original weights and biases for FC6 and FC7 were subsampled accordingly (i.e. the \"atrous\" layers referred to in the paper, this was found to make prediction ~20% faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate(tensor, m):\n",
    "    \"\"\"\n",
    "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
    "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
    "    :param tensor: tensor to be decimated\n",
    "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
    "    :return: decimated tensor\n",
    "    \n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == len(m)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d,\n",
    "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Base VGG module of SSD network\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "        # standard convolutional layers in VGG16\n",
    "        # conv1\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "        # conv4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
    "        # replacements for FC6 & FC7\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "        self.load_params()\n",
    "        \n",
    "        \n",
    "    def load_params(self):\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "        # pretrained VGG base\n",
    "        vgg = vgg16(True)\n",
    "        pretrained_state_dict = vgg.state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "        \n",
    "        # transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate([p for p in param_names if 'features' in p]):\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # conv1 fwd sequence\n",
    "        out = F.relu(self.conv1_1(image))\n",
    "        out = F.relu(self.conv1_2(out))\n",
    "        out = self.pool1(out)\n",
    "        # conv2 fwd sequence\n",
    "        out = F.relu(self.conv2_1(out))\n",
    "        out = F.relu(self.conv2_2(out))\n",
    "        out = self.pool2(out)\n",
    "        # conv3 fwd sequence\n",
    "        out = F.relu(self.conv3_1(out))\n",
    "        out = F.relu(self.conv3_2(out))\n",
    "        out = F.relu(self.conv3_3(out))\n",
    "        out = self.pool3(out)\n",
    "        # conv4 fwd sequence\n",
    "        out = F.relu(self.conv4_1(out))\n",
    "        out = F.relu(self.conv4_2(out))\n",
    "        out = F.relu(self.conv4_3(out))\n",
    "        out = self.pool4(out)\n",
    "        conv4_3_features = out\n",
    "        # conv5 fwd sequence\n",
    "        out = F.relu(self.conv5_1(out))\n",
    "        out = F.relu(self.conv5_2(out))\n",
    "        out = F.relu(self.conv5_3(out))\n",
    "        out = self.pool5(out)\n",
    "        # conv6\n",
    "        out = F.relu(self.conv6(out))\n",
    "        conv7_features = F.relu(self.conv7(out))\n",
    "        return conv4_3_features, conv7_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Feature Layers\n",
    "\n",
    "With the `VGGBase` module, we have constructed the layers up to `Conv7`; now we need to construct the remainder of the layers with `Conv8_2`, `Conv9_2`, `Conv10_2`, and `Conv11_2` that compose the \"Extra Feature Layers\" portion of the model, as shown in the architecture diagram above. These additional convolution layers provides higher-spatial feature maps of the input image. (Section 2.1 *Multi-scale feature maps for detection* part of the original SSD paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Auxiliary layers subsequent to the VGG base module of SSD\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AuxLayers, self).__init__()\n",
    "        # Conv8_2 layer components\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "        # Conv9_2 layer components\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "        # Conv10_2 layer components\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "        # Conv11_2 layer components\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "        # init layer parameters\n",
    "        self.init_conv2d()\n",
    "        \n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters with Xavier uniform\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "                \n",
    "    def forward(self, conv7_features):\n",
    "        # conv8 fwd sequences\n",
    "        out = F.relu(self.conv8_1(conv7_features))\n",
    "        out = F.relu(self.conv8_2(out))\n",
    "        conv8_2_ft = out\n",
    "        # conv9 fwd sequences\n",
    "        out = F.relu(self.conv9_1(out))\n",
    "        out = F.relu(self.conv9_2(out))\n",
    "        conv9_2_ft = out\n",
    "        # conv10 fwd sequences\n",
    "        out = F.relu(self.conv10_1(out))\n",
    "        out = F.relu(self.conv10_2(out))\n",
    "        conv10_2_ft = out\n",
    "        # conv11 fwd sequences\n",
    "        out = F.relu(self.conv11_1(out))\n",
    "        out = F.relu(self.conv11_2(out))\n",
    "        conv11_2_ft = out\n",
    "        return conv8_2_ft, conv9_2_ft, conv10_2_ft, conv11_2_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Layers\n",
    "\n",
    "This is the last sub-component within the overall SSD model. This module takes in an input feature map (from any one of `conv4_3`, `conv7`, `conv8_2`, `conv9_2`, `conv10_2`, `conv11_2`), where each position grid in the feature map reflect the corresponding physical position on the input image (albeit at different scale), and produces two set of predictions: \n",
    "\n",
    "a) bounding box offsets as represented by four numbers; and \n",
    "b) object class probabilities\n",
    "\n",
    "This makes the number of outputs to be `(4 + N-class categories)` at each of the feature map position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Prediction conv layers to output bound box output and class probabilities\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredLayers, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Define how many bounding boxes (with different aspect ratio)\n",
    "        # there to be per grid location\n",
    "        n_boxes = {'conv4_3' : 4,\n",
    "                   'conv7'   : 6,\n",
    "                   'conv8_2' : 6,\n",
    "                   'conv9_2' : 6,\n",
    "                   'conv10_2': 4,\n",
    "                   'conv11_2': 4}\n",
    "        \n",
    "        # Bounding box offset predictors\n",
    "        self.loc_conv4_3  = nn.Conv2d(512 , n_boxes['conv4_3']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv7    = nn.Conv2d(1024, n_boxes['conv7']*4,   kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2  = nn.Conv2d(512 , n_boxes['conv8_2']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2  = nn.Conv2d(256 , n_boxes['conv9_2']*4,  kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256 , n_boxes['conv10_2']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256 , n_boxes['conv11_2']*4, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Object class predictors\n",
    "        self.cl_conv4_3  = nn.Conv2d(512,  n_boxes['conv4_3'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv7    = nn.Conv2d(1024, n_boxes['conv7'] * n_classes,    kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2  = nn.Conv2d(512,  n_boxes['conv8_2'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2  = nn.Conv2d(256,  n_boxes['conv9_2'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256,  n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256,  n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Initalize all convolution parameters\n",
    "        self.init_conv2d()\n",
    "        \n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Init conv2d layers with Xavier norm and 0 bias\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "                \n",
    "    def forward(self, conv4_3_ft, conv7_ft, conv8_2_ft, conv9_2_ft, conv10_2_ft, conv11_2_ft):\n",
    "        batch_size = conv4_3_ft.size(0)\n",
    "\n",
    "        # Locator outputs for bounding box\n",
    "        # --------------------------------\n",
    "        # Conv4_3 locator\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_ft)             # (N, 16, 38, 38)\n",
    "        # note: contiguous() ensures tensor is stored in a contiguous \n",
    "        # chunk of memory; needed for calling .view() for reshaping below\n",
    "        l_conv4_3 = l_conv4_3.permute(0,2,3,1).contiguous()  # (N, 38, 38, 16)\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)        # (N, 5776, 4), total of 5776 bound boxes         \n",
    "        # Conv7 locator\n",
    "        l_conv7 = self.loc_conv7(conv7_ft)                   # (N, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0,2,3,1).contiguous()      # (N, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)            # (N, 2166, 4)        \n",
    "        # Conv8 locator\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_ft)             # (N, 24, 19, 19)\n",
    "        l_conv8_2 = l_conv8_2.permute(0,2,3,1).contiguous()  # (N, 19, 19, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)        # (N, 2166, 4)         \n",
    "        # Conv9 locator\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_ft)             # (N, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0,2,3,1).contiguous()  # (N, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)        # (N, 150, 4)        \n",
    "        # Conv10 locator\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_ft)            # (N, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0,2,3,1).contiguous()  # (N, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)        # (N, 150, 4)        \n",
    "        # Conv11 locator\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_ft)            # (N, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0,2,3,1).contiguous()  # (N, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)        # (N, 4, 4)\n",
    "        \n",
    "        # Class prediction outputs for each bounding box\n",
    "        # ----------------------------------------------\n",
    "        # Conv4_3 classifier\n",
    "        cl_conv4_3 = self.cl_conv4_3(conv4_3_ft)                       # (N, 4 boxes * n_classes, 38, 38)\n",
    "        cl_conv4_3 = cl_conv4_3.permute(0,2,3,1).contiguous()          # (N, 38, 38, 4 boxes * n_classes)\n",
    "        cl_conv4_3 = cl_conv4_3.view(batch_size, -1, self.n_classes)   # (N, 5776, n_classes)\n",
    "        # Conv7 classifier\n",
    "        cl_conv7   = self.cl_conv7(conv7_ft)                           # (N, 6 boxes * n_classes, 19, 19)\n",
    "        cl_conv7   = cl_conv7.permute(0,2,3,1).contiguous()            # (N, 19, 19, 6 boxes * n_classes)\n",
    "        cl_conv7   = cl_conv7.view(batch_size, -1, self.n_classes)     # (N, 2166, n_classes)\n",
    "        # Conv8_2 classifier\n",
    "        cl_conv8_2 = self.cl_conv8_2(conv8_2_ft)                       # (N, 6 boxes * n_classes, 10, 10)\n",
    "        cl_conv8_2 = cl_conv8_2.permute(0,2,3,1).contiguous()          # (N, 10, 10, 6 boxes * n_classes)\n",
    "        cl_conv8_2 = cl_conv8_2.view(batch_size, -1, self.n_classes)   # (N, 600, n_classes)\n",
    "        # Conv9_2 classifier\n",
    "        cl_conv9_2 = self.cl_conv9_2(conv9_2_ft)                       # (N, 6 boxes * n_classes, 5, 5)\n",
    "        cl_conv9_2 = cl_conv9_2.permute(0,2,3,1).contiguous()          # (N, 5, 5, 6 boxes * n_classes)\n",
    "        cl_conv9_2 = cl_conv9_2.view(batch_size, -1, self.n_classes)   # (N, 150, n_classes)\n",
    "        # Conv10_2 classifier\n",
    "        cl_conv10_2 = self.cl_conv10_2(conv10_2_ft)                    # (N, 4 boxes * n_classes, 3, 3)\n",
    "        cl_conv10_2 = cl_conv10_2.permute(0,2,3,1).contiguous()        # (N, 3, 3, 4 boxes * n_classes)\n",
    "        cl_conv10_2 = cl_conv10_2.view(batch_size, -1, self.n_classes) # (N, 36, n_classes)\n",
    "        # Conv11_2 classifier\n",
    "        cl_conv11_2 = self.cl_conv11_2(conv11_2_ft)                    # (N, 4 boxes * n_classes, 1, 1)\n",
    "        cl_conv11_2 = cl_conv11_2.permute(0,2,3,1).contiguous()        # (N, 1, 1, 4 boxes * n_classes)\n",
    "        cl_conv11_2 = cl_conv11_2.view(batch_size, -1, self.n_classes) # (N, 4, n_classes)  \n",
    "        \n",
    "        # Concatenate all locators and all classifiers\n",
    "        # There are a total of 5776 + 2166 + 600 + 150 + 36 + 4 = 8732 bounding box locations in total\n",
    "        locations = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
    "        class_scores = torch.cat([cl_conv4_3, cl_conv7, cl_conv8_2, cl_conv9_2, cl_conv10_2, cl_conv11_2], dim=1)\n",
    "        \n",
    "        return locations, class_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD300 Network\n",
    "\n",
    "Finally, the SSD300 network combines the `VGGBase`, `AuxLayers` and `PredLayers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD300(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD300, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        # network components\n",
    "        self.base = VGGBase()\n",
    "        self.aux  = AuxLayers()\n",
    "        self.pred = PredLayers(self.n_classes)\n",
    "        # rescale factor \n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))\n",
    "        nn.init.constant_(self.rescale_factors, 20) # init values to 20\n",
    "        # create prior boxes\n",
    "        self.prior_boxes = self.create_prior_boxes()\n",
    "        \n",
    "        \n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
    "        \"\"\"\n",
    "        # size of kernels in each respective feature maps\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "        \n",
    "        # relative scale of each feature map to the input image\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        # different aspect ratio bounding boxes to use at each feature map layer\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "        \n",
    "        fmaps = list(fmap_dims.keys())\n",
    "        prior_boxes = []\n",
    "\n",
    "        # iterate through each feature map\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            \n",
    "            # go through each grid-location on the feature map (i, j)\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    \n",
    "                    # compute bounding box center coordinates normalized against the size of feature map dimension\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "                    \n",
    "                    # populate bounding boxes of different aspect ratio to prior_boxes list\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        # bounding boxes defined in terms [center_x_coord, center_y_coord, w, h]\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
    "                        # scale of the current feature map and the scale of the next feature map\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            # For the last feature map, there is no \"next\" feature map (i.e. index out of bound in fmaps[k+1]) \n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # shape (8732, 4)\n",
    "        prior_boxes.clamp_(0, 1) # truncate all values between [0,1]\n",
    "\n",
    "        return prior_boxes\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        # Run VGG base network convolutions (lower level feature map generators)\n",
    "        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n",
    "\n",
    "        # Rescale conv4_3 after L2 norm\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n",
    "\n",
    "        # Run auxiliary convolutions (higher level feature map generators)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n",
    "            self.aux(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
    "\n",
    "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
    "        locs, classes_scores = self.pred(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
    "                                         conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass\n",
    "\n",
    "Now that the SSD300 model architecture was fully defined, lets try forward pass on a single batch of image to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.69s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# define the sequence of transformations to apply to each image sample \n",
    "basic_tfs = [PhotometricDistort(1.),\n",
    "             Flip(0.5),\n",
    "             ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "             Zoomout(0.5, max_scale=2.5),\n",
    "             Normalize(), \n",
    "             Resize((300,300))]\n",
    "tfms = transforms.Compose(basic_tfs)\n",
    "\n",
    "# instantiate the dataset object\n",
    "ds = CocoDataset(data_dir='./', dataset='val2017', anno_type='instances', transforms=tfms)\n",
    "\n",
    "# create dataloader\n",
    "BS = 8\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True, \n",
    "                collate_fn=partial(ds.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD300(\n",
       "  (base): VGGBase(\n",
       "    (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool5): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "    (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxLayers(\n",
       "    (conv8_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv8_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv9_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv9_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv10_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv10_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv11_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv11_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (pred): PredLayers(\n",
       "    (loc_conv4_3): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv7): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv8_2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv9_2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv10_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv11_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv4_3): Conv2d(512, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv7): Conv2d(1024, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv8_2): Conv2d(512, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv9_2): Conv2d(256, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv10_2): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv11_2): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model object\n",
    "ssd = SSD300(len(ds.allcats))\n",
    "ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image batch tensor shape: torch.Size([8, 3, 300, 300])\n",
      "bounding box location prediction shape: torch.Size([8, 4400, 4])\n",
      "object class prediction shape: torch.Size([8, 4400, 80])\n"
     ]
    }
   ],
   "source": [
    "# test forward pass for one batch\n",
    "for batch in dl:\n",
    "    image_batch = batch['images']\n",
    "    print(f\"image batch tensor shape: {image_batch.size()}\")\n",
    "    # forward pass through SSD300\n",
    "    locs, cls_scores = ssd(image_batch)\n",
    "    print(f\"bounding box location prediction shape: {locs.size()}\")\n",
    "    print(f\"object class prediction shape: {cls_scores.size()}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
