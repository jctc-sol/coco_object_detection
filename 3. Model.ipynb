{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from torch import nn\n",
    "from torchvision.models import vgg16\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "basic_tfs = [PhotometricDistort(1.),\n",
    "             Flip(0.5),\n",
    "             Resize((300,300)),\n",
    "             ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "             Zoomout(0.5, max_scale=2.5),\n",
    "             Normalize()]\n",
    "tfms = transforms.Compose(basic_tfs)\n",
    "ds = CocoDataset(data_dir='./', dataset='val2017', anno_type='instances', transforms=tfms)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "The model architecture for the Single Shot MultiBox Detector (SSD) is shown below (taken from the original SSD paper).\n",
    "![SSD architecture from arXiv:1512.02325v5](./illustrations/SSD_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Base\n",
    "\n",
    "The first starting point is to leverage the pre-trained VGG-16 model (without batch normalizations) for the SSD backbone. All convolution and pooling layers are kept under the \"features\" sequential portion of VGG-16 (i.e. up to `Conv5_3` layer as show in the diagram above). \n",
    "\n",
    "The SSD architecture then converted the fully connected layers `FC6` and `FC7` to convolution layers. In order to get the shapes to align, the original weights and biases for FC6 and FC7 were subsampled accordingly (i.e. the \"atrous\" layers referred to in the paper, this was found to make prediction ~20% faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate(tensor, m):\n",
    "    \"\"\"\n",
    "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
    "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
    "    :param tensor: tensor to be decimated\n",
    "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
    "    :return: decimated tensor\n",
    "    \n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == len(m)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d,\n",
    "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Base VGG module of SSD network\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "        # standard convolutional layers in VGG16\n",
    "        # conv1\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "        # conv4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
    "        # replacements for FC6 & FC7\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "        self.load_params()\n",
    "        \n",
    "        \n",
    "    def load_params(self):\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "        # pretrained VGG base\n",
    "        vgg = vgg16(True)\n",
    "        pretrained_state_dict = vgg.state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "        \n",
    "        # transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate([p for p in param_names if 'features' in p]):\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # conv1 fwd sequence\n",
    "        out = F.relu(self.conv1_1(image))\n",
    "        out = F.relu(self.conv1_2(out))\n",
    "        out = self.pool1(out)\n",
    "        # conv2 fwd sequence\n",
    "        out = F.relu(self.conv2_1(out))\n",
    "        out = F.relu(self.conv2_2(out))\n",
    "        out = self.pool2(out)\n",
    "        # conv3 fwd sequence\n",
    "        out = F.relu(self.conv3_1(out))\n",
    "        out = F.relu(self.conv3_2(out))\n",
    "        out = F.relu(self.conv3_3(out))\n",
    "        out = self.pool3(out)\n",
    "        # conv4 fwd sequence\n",
    "        out = F.relu(self.conv4_1(out))\n",
    "        out = F.relu(self.conv4_2(out))\n",
    "        out = F.relu(self.conv4_3(out))\n",
    "        out = self.pool4(out)\n",
    "        conv4_3_features = out\n",
    "        # conv5 fwd sequence\n",
    "        out = F.relu(self.conv5_1(out))\n",
    "        out = F.relu(self.conv5_2(out))\n",
    "        out = F.relu(self.conv5_3(out))\n",
    "        out = self.pool5(out)\n",
    "        # conv6\n",
    "        out = F.relu(self.conv6(out))\n",
    "        conv7_features = F.relu(self.conv7(out))\n",
    "        return conv4_3_features, conv7_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Feature Layers\n",
    "\n",
    "With the `VGGBase` module, we have constructed the layers up to `Conv7`; now we need to construct the remainder of the layers with `Conv8_2`, `Conv9_2`, `Conv10_2`, and `Conv11_2` that compose the \"Extra Feature Layers\" portion of the model, as shown in the architecture diagram above. These additional convolution layers provides higher-spatial feature maps of the input image. (Section 2.1 *Multi-scale feature maps for detection* part of the original SSD paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Auxiliary layers subsequent to the VGG base module of SSD\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AuxLayers, self).__init__()\n",
    "        # Conv8_2 layer components\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "        # Conv9_2 layer components\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "        # Conv10_2 layer components\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "        # Conv11_2 layer components\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "        # init layer parameters\n",
    "        self.init_conv2d()\n",
    "        \n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters with Xavier uniform\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "                \n",
    "    def forward(self, conv7_features):\n",
    "        # conv8 fwd sequences\n",
    "        out = F.relu(self.conv8_1(conv7_features))\n",
    "        out = F.relu(self.conv8_2(out))\n",
    "        conv8_2_ft = out\n",
    "        # conv9 fwd sequences\n",
    "        out = F.relu(self.conv9_1(out))\n",
    "        out = F.relu(self.conv9_2(out))\n",
    "        conv9_2_ft = out\n",
    "        # conv10 fwd sequences\n",
    "        out = F.relu(self.conv10_1(out))\n",
    "        out = F.relu(self.conv10_2(out))\n",
    "        conv10_2_ft = out\n",
    "        # conv11 fwd sequences\n",
    "        out = F.relu(self.conv11_1(out))\n",
    "        out = F.relu(self.conv11_2(out))\n",
    "        conv11_2_ft = out\n",
    "        return conv8_2_ft, conv9_2_ft, conv10_2_ft, conv11_2_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Layers\n",
    "\n",
    "This is the last sub-component within the overall SSD model. This module takes in an input feature map (from any one of `conv4_3`, `conv7`, `conv8_2`, `conv9_2`, `conv10_2`, `conv11_2`), where each position grid in the feature map reflect the corresponding physical position on the input image (albeit at different scale), and produces two set of predictions: \n",
    "\n",
    "a) bounding box offsets as represented by four numbers; and \n",
    "b) object class probabilities\n",
    "\n",
    "This makes the number of outputs to be `(4 + N-class categories)` at each of the feature map position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Prediction conv layers to output bound box output and class probabilities\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredLayers, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Define how many bounding boxes (with different aspect ratio)\n",
    "        # there to be per grid location\n",
    "        n_boxes = {'conv4_3' : 4,\n",
    "                   'conv7'   : 6,\n",
    "                   'conv8_2' : 6,\n",
    "                   'conv9_2' : 6,\n",
    "                   'conv10_2': 4,\n",
    "                   'conv11_2': 4}\n",
    "        \n",
    "        # Bounding box offset predictors\n",
    "        self.loc_conv4_3  = nn.Conv2d(512 , n_boxes['conv4_3']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv7    = nn.Conv2d(1024, n_boxes['conv7']*4,   kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2  = nn.Conv2d(512 , n_boxes['conv8_2']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2  = nn.Conv2d(256 , n_boxes['conv9_2']*4,  kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256 , n_boxes['conv10_2']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256 , n_boxes['conv11_2']*4, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Object class predictors\n",
    "        self.cl_conv4_3  = nn.Conv2d(512,  n_boxes['conv4_3'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv7    = nn.Conv2d(1024, n_boxes['conv7'] * n_classes,    kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2  = nn.Conv2d(512,  n_boxes['conv8_2'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2  = nn.Conv2d(256,  n_boxes['conv9_2'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256,  n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256,  n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Initalize all convolution parameters\n",
    "        self.init_conv2d()\n",
    "        \n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Init conv2d layers with Xavier norm and 0 bias\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "                \n",
    "    def forward(self, conv4_3_ft, conv7_ft, conv8_2_ft, conv9_2_ft, conv10_2_ft, conv11_2_ft):\n",
    "        bs = conv4_3_ft.size(0)\n",
    "\n",
    "        # Locator outputs for bounding box\n",
    "        # --------------------------------\n",
    "        # Conv4_3 locator\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_ft)             # (N, 16, 38, 38)\n",
    "        # note: contiguous() ensures tensor is stored in a contiguous \n",
    "        # chunk of memory; needed for calling .view() for reshaping below\n",
    "        l_conv4_3 = l_conv4_3.permute(0,2,3,1).contiguous()  # (N, 38, 38, 16)\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)        # (N, 5776, 4), total of 5776 bound boxes         \n",
    "        # Conv7 locator\n",
    "        l_conv7 = self.loc_conv7(conv7_ft)                   # (N, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0,2,3,1).contiguous()      # (N, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)            # (N, 2166, 4)        \n",
    "        # Conv8 locator\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_ft)             # (N, 24, 19, 19)\n",
    "        l_conv8_2 = l_conv8_2.permute(0,2,3,1).contiguous()  # (N, 19, 19, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)        # (N, 2166, 4)         \n",
    "        # Conv9 locator\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_ft)             # (N, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0,2,3,1).contiguous()  # (N, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)        # (N, 150, 4)        \n",
    "        # Conv10 locator\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_ft)            # (N, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0,2,3,1).contiguous()  # (N, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)        # (N, 150, 4)        \n",
    "        # Conv11 locator\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_ft)            # (N, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0,2,3,1).contiguous()  # (N, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)        # (N, 4, 4)\n",
    "        \n",
    "        # Class prediction outputs for each bounding box\n",
    "        # ----------------------------------------------\n",
    "        # Conv4_3 classifier\n",
    "        cl_conv4_3 = self.cl_conv4_3(conv4_3_ft)                       # (N, 4 boxes * n_classes, 38, 38)\n",
    "        cl_conv4_3 = cl_conv4_3.permute(0,2,3,1).contiguous()          # (N, 38, 38, 4 boxes * n_classes)\n",
    "        cl_conv4_3 = cl_conv4_3.view(batch_size, -1, self.n_classes)   # (N, 5776, n_classes)\n",
    "        # Conv7 classifier\n",
    "        cl_conv7   = self.cl_conv7(conv7_ft)                           # (N, 6 boxes * n_classes, 19, 19)\n",
    "        cl_conv7   = cl_conv7.permute(0,2,3,1).contiguous()            # (N, 19, 19, 6 boxes * n_classes)\n",
    "        cl_conv7   = cl_conv7.view(batch_size, -1, self.n_classes)     # (N, 2166, n_classes)\n",
    "        # Conv8_2 classifier\n",
    "        cl_conv8_2 = self.cl_conv8_2(conv8_2_ft)                       # (N, 6 boxes * n_classes, 10, 10)\n",
    "        cl_conv8_2 = cl_conv8_2.permute(0,2,3,1).contiguous()          # (N, 10, 10, 6 boxes * n_classes)\n",
    "        cl_conv8_2 = cl_conv8_2.view(batch_size, -1, self.n_classes)   # (N, 600, n_classes)\n",
    "        # Conv9_2 classifier\n",
    "        cl_conv9_2 = self.cl_conv9_2(conv9_2_ft)                       # (N, 6 boxes * n_classes, 5, 5)\n",
    "        cl_conv9_2 = cl_conv9_2.permute(0,2,3,1).contiguous()          # (N, 5, 5, 6 boxes * n_classes)\n",
    "        cl_conv9_2 = cl_conv9_2.view(batch_size, -1, self.n_classes)   # (N, 150, n_classes)\n",
    "        # Conv10_2 classifier\n",
    "        cl_conv10_2 = self.cl_conv10_2(conv10_2_ft)                    # (N, 4 boxes * n_classes, 3, 3)\n",
    "        cl_conv10_2 = cl_conv10_2.permute(0,2,3,1).contiguous()        # (N, 3, 3, 4 boxes * n_classes)\n",
    "        cl_conv10_2 = cl_conv10_2.view(batch_size, -1, self.n_classes) # (N, 36, n_classes)\n",
    "        # Conv11_2 classifier\n",
    "        cl_conv11_2 = self.cl_conv11_2(conv11_2_ft)                    # (N, 4 boxes * n_classes, 1, 1)\n",
    "        cl_conv11_2 = cl_conv11_2.permute(0,2,3,1).contiguous()        # (N, 1, 1, 4 boxes * n_classes)\n",
    "        cl_conv11_2 = cl_conv11_2.view(batch_size, -1, self.n_classes) # (N, 4, n_classes)  \n",
    "        \n",
    "        # Concatenate all locators and all classifiers\n",
    "        # There are a total of 5776 + 2166 + 600 + 150 + 36 + 4 = 8732 box locations in total\n",
    "        locations = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
    "        class_scores = torch.cat([cl_conv4_3, cl_conv7, cl_conv8_2, cl_conv9_2, cl_conv10_2, cl_conv11_2], dim=1)\n",
    "        \n",
    "        return locations, class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = VGGBase()\n",
    "aux  = AuxLayers()\n",
    "pred = PredLayers(n_classes=len(ds.allcats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
