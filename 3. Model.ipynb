{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from torchvision.models import vgg16\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from math import sqrt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "The model architecture for the Single Shot MultiBox Detector (SSD) is shown below (taken from the original SSD paper).\n",
    "![SSD architecture from arXiv:1512.02325v5](./illustrations/SSD_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Base\n",
    "\n",
    "The first starting point is to leverage the pre-trained VGG-16 model (without batch normalizations) for the SSD backbone. All convolution and pooling layers are kept under the \"features\" sequential portion of VGG-16 (i.e. up to `Conv5_3` layer as show in the diagram above). \n",
    "\n",
    "The SSD architecture then converted the fully connected layers `FC6` and `FC7` to convolution layers. In order to get the shapes to align, the original weights and biases for FC6 and FC7 were subsampled accordingly (i.e. the \"atrous\" layers referred to in the paper, this was found to make prediction ~20% faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate(tensor, m):\n",
    "    \"\"\"\n",
    "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
    "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
    "    :param tensor: tensor to be decimated\n",
    "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
    "    :return: decimated tensor\n",
    "    \n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == len(m)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d,\n",
    "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Base VGG module of SSD network\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "        # standard convolutional layers in VGG16\n",
    "        # conv1\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1   = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "        # conv4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # conv5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
    "        # replacements for FC6 & FC7\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "        # init layer parameters\n",
    "        self.load_params()\n",
    "        \n",
    "        \n",
    "    def load_params(self):\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "        # pretrained VGG base\n",
    "        vgg = vgg16(True)\n",
    "        pretrained_state_dict = vgg.state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "        \n",
    "        # transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate([p for p in param_names if 'features' in p]):\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # conv1 fwd sequence\n",
    "        out = F.relu(self.conv1_1(image))\n",
    "        out = F.relu(self.conv1_2(out))\n",
    "        out = self.pool1(out)\n",
    "        # conv2 fwd sequence\n",
    "        out = F.relu(self.conv2_1(out))\n",
    "        out = F.relu(self.conv2_2(out))\n",
    "        out = self.pool2(out)\n",
    "        # conv3 fwd sequence\n",
    "        out = F.relu(self.conv3_1(out))\n",
    "        out = F.relu(self.conv3_2(out))\n",
    "        out = F.relu(self.conv3_3(out))\n",
    "        out = self.pool3(out)\n",
    "        # conv4 fwd sequence\n",
    "        out = F.relu(self.conv4_1(out))\n",
    "        out = F.relu(self.conv4_2(out))\n",
    "        out = F.relu(self.conv4_3(out))\n",
    "        conv4_3_features = out\n",
    "        out = self.pool4(out)\n",
    "        # conv5 fwd sequence\n",
    "        out = F.relu(self.conv5_1(out))\n",
    "        out = F.relu(self.conv5_2(out))\n",
    "        out = F.relu(self.conv5_3(out))\n",
    "        out = self.pool5(out)\n",
    "        # conv6\n",
    "        out = F.relu(self.conv6(out))\n",
    "        conv7_features = F.relu(self.conv7(out))\n",
    "        return conv4_3_features, conv7_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Feature Layers\n",
    "\n",
    "With the `VGGBase` module, we have constructed the layers up to `Conv7`; now we need to construct the remainder of the layers with `Conv8_2`, `Conv9_2`, `Conv10_2`, and `Conv11_2` that compose the \"Extra Feature Layers\" portion of the model, as shown in the architecture diagram above. These additional convolution layers provides higher-spatial feature maps of the input image. (Section 2.1 *Multi-scale feature maps for detection* part of the original SSD paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Auxiliary layers subsequent to the VGG base module of SSD\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AuxLayers, self).__init__()\n",
    "        # Conv8_2 layer components\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "        # Conv9_2 layer components\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "        # Conv10_2 layer components\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "        # Conv11_2 layer components\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "        # init layer parameters\n",
    "        self.init_conv2d()\n",
    "        \n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters with Xavier uniform\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "                \n",
    "    def forward(self, conv7_features):\n",
    "        # conv8 fwd sequences\n",
    "        out = F.relu(self.conv8_1(conv7_features))\n",
    "        out = F.relu(self.conv8_2(out))\n",
    "        conv8_2_ft = out\n",
    "        # conv9 fwd sequences\n",
    "        out = F.relu(self.conv9_1(out))\n",
    "        out = F.relu(self.conv9_2(out))\n",
    "        conv9_2_ft = out\n",
    "        # conv10 fwd sequences\n",
    "        out = F.relu(self.conv10_1(out))\n",
    "        out = F.relu(self.conv10_2(out))\n",
    "        conv10_2_ft = out\n",
    "        # conv11 fwd sequences\n",
    "        out = F.relu(self.conv11_1(out))\n",
    "        out = F.relu(self.conv11_2(out))\n",
    "        conv11_2_ft = out\n",
    "        return conv8_2_ft, conv9_2_ft, conv10_2_ft, conv11_2_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Layers\n",
    "\n",
    "This is the last sub-component within the overall SSD model. This module takes in an input feature map (from any one of `conv4_3`, `conv7`, `conv8_2`, `conv9_2`, `conv10_2`, `conv11_2`), where each position grid in the feature map reflect the corresponding physical position on the input image (albeit at different scale), and produces two set of predictions: \n",
    "\n",
    "a) bounding box offsets as represented by four numbers; and \n",
    "\n",
    "b) object class probabilities\n",
    "\n",
    "This makes the number of outputs to be `(4 + N-class categories)` at each of the feature map position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Prediction conv layers to output bound box output and class probabilities\n",
    "    Code: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredLayers, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Define how many bounding boxes (with different aspect ratio)\n",
    "        # there to be per grid location\n",
    "        n_boxes = {'conv4_3' : 4,\n",
    "                   'conv7'   : 6,\n",
    "                   'conv8_2' : 6,\n",
    "                   'conv9_2' : 6,\n",
    "                   'conv10_2': 4,\n",
    "                   'conv11_2': 4}\n",
    "        \n",
    "        # Bounding box offset predictors\n",
    "        self.loc_conv4_3  = nn.Conv2d(512 , n_boxes['conv4_3']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv7    = nn.Conv2d(1024, n_boxes['conv7']*4,   kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2  = nn.Conv2d(512 , n_boxes['conv8_2']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2  = nn.Conv2d(256 , n_boxes['conv9_2']*4,  kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256 , n_boxes['conv10_2']*4, kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256 , n_boxes['conv11_2']*4, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Object class predictors\n",
    "        self.cl_conv4_3  = nn.Conv2d(512,  n_boxes['conv4_3'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv7    = nn.Conv2d(1024, n_boxes['conv7'] * n_classes,    kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2  = nn.Conv2d(512,  n_boxes['conv8_2'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2  = nn.Conv2d(256,  n_boxes['conv9_2'] * n_classes,  kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256,  n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256,  n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Initalize all convolution parameters\n",
    "        self.init_conv2d()\n",
    "        \n",
    "    \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Init conv2d layers with Xavier norm and 0 bias\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "                \n",
    "                \n",
    "    def forward(self, conv4_3_ft, conv7_ft, conv8_2_ft, conv9_2_ft, conv10_2_ft, conv11_2_ft):\n",
    "        batch_size = conv4_3_ft.size(0)\n",
    "\n",
    "        # Locator outputs for bounding box\n",
    "        # --------------------------------\n",
    "        # Conv4_3 locator\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_ft)             # (batch, 16, 38, 38)\n",
    "        # note: contiguous() ensures tensor is stored in a contiguous \n",
    "        # chunk of memory; needed for calling .view() for reshaping below\n",
    "        l_conv4_3 = l_conv4_3.permute(0,2,3,1).contiguous()  # (batch, 38, 38, 16)\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)        # (batch, 5776, 4), total of 5776 bound boxes         \n",
    "        # Conv7 locator\n",
    "        l_conv7 = self.loc_conv7(conv7_ft)                   # (batch, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0,2,3,1).contiguous()      # (batch, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)            # (batch, 2166, 4)        \n",
    "        # Conv8 locator\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_ft)             # (batch, 24, 19, 19)\n",
    "        l_conv8_2 = l_conv8_2.permute(0,2,3,1).contiguous()  # (batch, 19, 19, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)        # (batch, 2166, 4)         \n",
    "        # Conv9 locator\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_ft)             # (batch, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0,2,3,1).contiguous()  # (batch, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)        # (batch, 150, 4)        \n",
    "        # Conv10 locator\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_ft)            # (batch, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0,2,3,1).contiguous()  # (batch, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)        # (batch, 150, 4)        \n",
    "        # Conv11 locator\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_ft)            # (batch, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0,2,3,1).contiguous()  # (batch, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)        # (batch, 4, 4)\n",
    "        \n",
    "        # Class prediction outputs for each bounding box\n",
    "        # ----------------------------------------------\n",
    "        # Conv4_3 classifier\n",
    "        cl_conv4_3 = self.cl_conv4_3(conv4_3_ft)                       # (N, 4 boxes * n_classes, 38, 38)\n",
    "        cl_conv4_3 = cl_conv4_3.permute(0,2,3,1).contiguous()          # (N, 38, 38, 4 boxes * n_classes)\n",
    "        cl_conv4_3 = cl_conv4_3.view(batch_size, -1, self.n_classes)   # (N, 5776, n_classes)\n",
    "        # Conv7 classifier\n",
    "        cl_conv7   = self.cl_conv7(conv7_ft)                           # (N, 6 boxes * n_classes, 19, 19)\n",
    "        cl_conv7   = cl_conv7.permute(0,2,3,1).contiguous()            # (N, 19, 19, 6 boxes * n_classes)\n",
    "        cl_conv7   = cl_conv7.view(batch_size, -1, self.n_classes)     # (N, 2166, n_classes)\n",
    "        # Conv8_2 classifier\n",
    "        cl_conv8_2 = self.cl_conv8_2(conv8_2_ft)                       # (N, 6 boxes * n_classes, 10, 10)\n",
    "        cl_conv8_2 = cl_conv8_2.permute(0,2,3,1).contiguous()          # (N, 10, 10, 6 boxes * n_classes)\n",
    "        cl_conv8_2 = cl_conv8_2.view(batch_size, -1, self.n_classes)   # (N, 600, n_classes)\n",
    "        # Conv9_2 classifier\n",
    "        cl_conv9_2 = self.cl_conv9_2(conv9_2_ft)                       # (N, 6 boxes * n_classes, 5, 5)\n",
    "        cl_conv9_2 = cl_conv9_2.permute(0,2,3,1).contiguous()          # (N, 5, 5, 6 boxes * n_classes)\n",
    "        cl_conv9_2 = cl_conv9_2.view(batch_size, -1, self.n_classes)   # (N, 150, n_classes)\n",
    "        # Conv10_2 classifier\n",
    "        cl_conv10_2 = self.cl_conv10_2(conv10_2_ft)                    # (N, 4 boxes * n_classes, 3, 3)\n",
    "        cl_conv10_2 = cl_conv10_2.permute(0,2,3,1).contiguous()        # (N, 3, 3, 4 boxes * n_classes)\n",
    "        cl_conv10_2 = cl_conv10_2.view(batch_size, -1, self.n_classes) # (N, 36, n_classes)\n",
    "        # Conv11_2 classifier\n",
    "        cl_conv11_2 = self.cl_conv11_2(conv11_2_ft)                    # (N, 4 boxes * n_classes, 1, 1)\n",
    "        cl_conv11_2 = cl_conv11_2.permute(0,2,3,1).contiguous()        # (N, 1, 1, 4 boxes * n_classes)\n",
    "        cl_conv11_2 = cl_conv11_2.view(batch_size, -1, self.n_classes) # (N, 4, n_classes)  \n",
    "        \n",
    "        # Concatenate all locators and all classifiers\n",
    "        # There are a total of 5776 + 2166 + 600 + 150 + 36 + 4 = 8732 bounding box locations in total\n",
    "        locations = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)\n",
    "        class_scores = torch.cat([cl_conv4_3, cl_conv7, cl_conv8_2, cl_conv9_2, cl_conv10_2, cl_conv11_2], dim=1)\n",
    "        \n",
    "        return locations, class_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD300 Network\n",
    "\n",
    "Finally, the SSD300 network combines the `VGGBase`, `AuxLayers` and `PredLayers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD300(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD300, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        # network components\n",
    "        self.base = VGGBase()\n",
    "        self.aux  = AuxLayers()\n",
    "        self.pred = PredLayers(self.n_classes)\n",
    "        # rescale factor \n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))\n",
    "        nn.init.constant_(self.rescale_factors, 20) # init values to 20\n",
    "        # create prior boxes\n",
    "        self.prior_boxes = self.create_prior_boxes()\n",
    "        # instantiate a coordinate transformation object to decipher object location\n",
    "        # output in prior box offset coordinate format to center coordinate format\n",
    "        self.oc2cc = OffsetCoord()\n",
    "        # instantiate a coordinate transformation object to decipher object location\n",
    "        # output in center coordinate format to boundary box coordinate format\n",
    "        self.cc2bc = BoundaryCoord()\n",
    "        \n",
    "        \n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
    "        \"\"\"\n",
    "        # size of kernels in each respective feature maps\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "        \n",
    "        # relative scale of each feature map to the input image\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        # different aspect ratio bounding boxes to use at each feature map layer\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "        \n",
    "        fmaps = list(fmap_dims.keys())\n",
    "        prior_boxes = []\n",
    "\n",
    "        # iterate through each feature map\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            \n",
    "            # go through each grid-location on the feature map (i, j)\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    \n",
    "                    # compute bounding box center coordinates normalized against the size of feature map dimension\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "                    \n",
    "                    # populate bounding boxes of different aspect ratio to prior_boxes list\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        # bounding boxes defined in terms [center_x_coord, center_y_coord, center_w_coord, center_h_coord]\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
    "                        # scale of the current feature map and the scale of the next feature map\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            # For the last feature map, there is no \"next\" feature map (i.e. index out of bound in fmaps[k+1]) \n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # shape (8732, 4)\n",
    "        prior_boxes.clamp_(0, 1) # truncate all values between [0,1]\n",
    "\n",
    "        return prior_boxes    \n",
    "    \n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run VGG base network convolutions (lower level feature map generators)\n",
    "        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n",
    "\n",
    "        # Rescale conv4_3 after L2 norm\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n",
    "\n",
    "        # Run auxiliary convolutions (higher level feature map generators)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n",
    "            self.aux(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
    "\n",
    "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
    "        locs, classes_scores = self.pred(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
    "                                         conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores\n",
    "\n",
    "\n",
    "    def detect_objects(self, predicted_boxes, predicted_scores, min_score_threshold, max_overlap_threshold, top_k):\n",
    "        \"\"\"\n",
    "        Post-process the prediction from the SSD output (from `forward` method) that apply Non-Maximum Suppression (NMS)\n",
    "        based on `min_score`, `max_overlap`, and `top_k` criteria to reduce the number of prior-bound boxes that are then\n",
    "        the formal output of `pred_locs`, 'pred_scores' and `pred_classes` from the SSD.\n",
    "        \n",
    "        For each of the below, M represents the `batch_size`, `n_i` is the number of predicted objects in each image, and \n",
    "        `N_i` is the number of true objects in each image.\n",
    "        \n",
    "        :param predicted_boxes: predicted locations/boxes w.r.t the 8732 prior bounding boxes, a tensor of dimensions \n",
    "                               (M, 8732, 4) in center coordinates\n",
    "        :param predicted_scores: predicted class scores for each of the 8732 prior bounding box locations, a tensor of \n",
    "                                 dimensions (M, 8732, n_classes)\n",
    "        :param min_score_threshold: minimum score threshold to apply against the class score for a prior bounding box to be \n",
    "                                    considered a match for a certain class\n",
    "        :param max_overlap_threshold: maximum overlap ratio two boxes can have so that the one with the lower score is not \n",
    "                                      suppressed via NMS\n",
    "        :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n",
    "\n",
    "        :return: detected_boxes: M length list of tensors (n_i, 4) for detected bounding boxes after NMS\n",
    "        :return: detected_labels: M length list of labels (n_i, n_classes) for detected class labels\n",
    "        :return: detected_scores: `batch_size` length list of scores (n_i, n_classes) \n",
    "        \n",
    "        Source ref: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py#L426\n",
    "        \"\"\"\n",
    "        batch_size = predicted_boxes.size(0)\n",
    "        n_priors = self.prior_boxes.size(0)\n",
    "        predicted_scores = F.softmax(predicted_scores, dim=2)  # apply softmax normalization across the class scores\n",
    "\n",
    "        # Lists to store final predicted boxes, labels, and scores for all images\n",
    "        detected_boxes  = list()\n",
    "        detected_labels = list()\n",
    "        detected_scores = list()\n",
    "\n",
    "        # ensure # of prior boxes align across input location & score predictions\n",
    "        assert n_priors == predicted_boxes.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        # iterate through each image in the batch\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # Init several lists to store boxes and scores for this image\n",
    "            image_boxes = list()\n",
    "            image_labels = list()\n",
    "            image_scores = list()\n",
    "\n",
    "            # model output of predicted boxes are natively in prior bounding box offset coordinate format,\n",
    "            # first decode it back to center box coordinate format, then from center box coordinate to\n",
    "            # boundary coordinate format\n",
    "            predicted_boxes_bc = self.cc2bc.encode(\n",
    "                self.oc2cc.encode(predicted_boxes[i], self.prior_boxes)\n",
    "            )  # size (8732, 4)\n",
    "            \n",
    "            # determine the most probable class & score from the softmax of predicted_scores\n",
    "            max_scores, pedicted_labels = predicted_scores[i].max(dim=1)  # size (8732)\n",
    "\n",
    "            # iterate through each class (except for class 0 which is reserved for background)\n",
    "            for c in range(1, self.n_classes):\n",
    "                \n",
    "                # get scores for all bounding boxes belonging to this class\n",
    "                class_scores = predicted_scores[i][:, c]  # size (8732)\n",
    "                \n",
    "                # apply score threshold to filter out low probabily ones\n",
    "                score_above_min_score = class_scores > min_score_threshold  # torch.uint8 (byte) tensor, for indexing\n",
    "                \n",
    "                # skip remainder steps if there are no scores above threshold\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                \n",
    "                # get scores & decoded box locations corresponding to the class\n",
    "                class_scores = class_scores[score_above_min_score]  # size (n_qualified); n_qualitfied = n_above_min_score\n",
    "                class_boxes  = predicted_boxes_bc[score_above_min_score]  # size (n_qualified, 4)\n",
    "                # sort according to score from highest to lowest\n",
    "                class_scores, sort_idx = class_scores.sort(dim=0, descending=True)\n",
    "                class_boxes = class_boxes[sort_idx]\n",
    "                \n",
    "                # compute jaccard overlap between all class boxes\n",
    "                overlap = find_jaccard_overlap(class_boxes, class_boxes) # size (n_qualified, n_qualified)\n",
    "\n",
    "                # Non-Maximum Suppression (NMS)\n",
    "                # init a torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
    "                # 1 implies suppress, 0 implies don't suppress\n",
    "                suppression_idx = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
    "\n",
    "                # Consider each box in order of decreasing scores\n",
    "                for box in range(class_boxes.size(0)):\n",
    "                    # If this box is already marked for suppression\n",
    "                    if suppression_idx[box] == 1:\n",
    "                        continue\n",
    "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
    "                    # Find such boxes and update suppress indices\n",
    "                    suppression_idx = torch.max(suppression_idx, overlap[box] > max_overlap_threshold)\n",
    "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
    "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
    "                    suppression_idx[box] = 0\n",
    "\n",
    "                # Store only unsuppressed boxes for this class\n",
    "                image_boxes.append(class_boxes[1 - suppression_idx])\n",
    "                image_labels.append(torch.LongTensor((1 - suppression_idx).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppression_idx])\n",
    "\n",
    "            # If no object in any class is found, store a placeholder for 'background'\n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "\n",
    "            # Concatenate into single tensors\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_qualified, 4)\n",
    "            image_labels = torch.cat(image_labels, dim=0)  # (n_qualified)\n",
    "            image_scores = torch.cat(image_scores, dim=0)  # (n_qualified)\n",
    "            n_objects = image_scores.size(0)\n",
    "\n",
    "            # Keep only the top k highest score objects\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]  # (top_k)\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
    "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
    "\n",
    "            # Append to lists that store predicted boxes and scores for all images\n",
    "            detected_boxes.append(image_boxes)\n",
    "            detected_labels.append(image_labels)\n",
    "            detected_scores.append(image_scores)\n",
    "\n",
    "        return detected_boxes, detected_labels, detected_scores  # lists of length batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass\n",
    "\n",
    "Now that the SSD300 model architecture was fully defined, lets try forward pass on a single batch of image to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# define the sequence of transformations to apply to each image sample \n",
    "basic_tfs = [PhotometricDistort(1.),\n",
    "             Flip(0.5),\n",
    "             ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "             Zoomout(0.5, max_scale=2.5),\n",
    "             Normalize(), \n",
    "             Resize((300,300))]\n",
    "tfms = transforms.Compose(basic_tfs)\n",
    "\n",
    "# instantiate the dataset object\n",
    "ds = CocoDataset(data_dir='./', dataset='val2017', anno_type='instances', transforms=tfms)\n",
    "\n",
    "# create dataloader\n",
    "BS = 8\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True, \n",
    "                collate_fn=partial(ds.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD300(\n",
       "  (base): VGGBase(\n",
       "    (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool5): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "    (conv7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxLayers(\n",
       "    (conv8_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv8_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv9_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv9_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (conv10_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv10_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv11_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv11_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (pred): PredLayers(\n",
       "    (loc_conv4_3): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv7): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv8_2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv9_2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv10_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (loc_conv11_2): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv4_3): Conv2d(512, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv7): Conv2d(1024, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv8_2): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv9_2): Conv2d(256, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv10_2): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cl_conv11_2): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model object\n",
    "ssd = SSD300(len(ds.id2cat))\n",
    "ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image batch tensor shape: torch.Size([8, 3, 300, 300])\n",
      "bounding box location prediction shape: torch.Size([8, 8732, 4])\n",
      "object class prediction shape: torch.Size([8, 8732, 81])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-2d830d57ccdd>:210: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1607370249289/work/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  image_boxes.append(class_boxes[1 - suppression_idx])\n",
      "<ipython-input-34-2d830d57ccdd>:212: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1607370249289/work/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  image_scores.append(class_scores[1 - suppression_idx])\n"
     ]
    }
   ],
   "source": [
    "# test forward pass for one batch\n",
    "for batch in dl:\n",
    "    image_batch = batch['images']\n",
    "    print(f\"image batch tensor shape: {image_batch.size()}\")\n",
    "    # forward pass through SSD300\n",
    "    pred_boxes, pred_scores = ssd(image_batch)\n",
    "    print(f\"bounding box location prediction shape: {pred_boxes.size()}\")\n",
    "    print(f\"object class prediction shape: {pred_scores.size()}\")\n",
    "    # use the predictions to detect objects\n",
    "    detected_boxes, detected_labels, detected_scores = ssd.detect_objects(\n",
    "        pred_boxes, pred_scores, min_score_threshold=0.1, max_overlap_threshold=0.5, top_k=10\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For image 0:\n",
      "Detected 10 object(s); \n",
      "with scores: tensor([0.1697, 0.1680, 0.1599, 0.1598, 0.1596, 0.1593, 0.1590, 0.1589, 0.1588,\n",
      "        0.1588], grad_fn=<SliceBackward>); and \n",
      "with predicted classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "For image 1:\n",
      "Detected 10 object(s); \n",
      "with scores: tensor([0.1703, 0.1686, 0.1607, 0.1605, 0.1601, 0.1596, 0.1595, 0.1594, 0.1591,\n",
      "        0.1591], grad_fn=<SliceBackward>); and \n",
      "with predicted classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "For image 2:\n",
      "Detected 10 object(s); \n",
      "with scores: tensor([0.1710, 0.1690, 0.1612, 0.1611, 0.1608, 0.1605, 0.1604, 0.1603, 0.1602,\n",
      "        0.1601], grad_fn=<SliceBackward>); and \n",
      "with predicted classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "For image 3:\n",
      "Detected 10 object(s); \n",
      "with scores: tensor([0.1693, 0.1680, 0.1587, 0.1587, 0.1583, 0.1582, 0.1582, 0.1582, 0.1582,\n",
      "        0.1582], grad_fn=<SliceBackward>); and \n",
      "with predicted classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "For image 4:\n",
      "Detected 10 object(s); \n",
      "with scores: tensor([0.1718, 0.1698, 0.1617, 0.1610, 0.1610, 0.1609, 0.1608, 0.1608, 0.1606,\n",
      "        0.1605], grad_fn=<SliceBackward>); and \n",
      "with predicted classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "For image 5:\n",
      "Detected 10 object(s); \n",
      "with scores: tensor([0.1703, 0.1692, 0.1594, 0.1593, 0.1591, 0.1589, 0.1587, 0.1586, 0.1586,\n",
      "        0.1586], grad_fn=<SliceBackward>); and \n",
      "with predicted classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "For image 6:\n",
      "Detected 10 object(s); \n",
      "with scores: tensor([0.1719, 0.1689, 0.1611, 0.1610, 0.1609, 0.1607, 0.1606, 0.1606, 0.1606,\n",
      "        0.1604], grad_fn=<SliceBackward>); and \n",
      "with predicted classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n",
      "For image 7:\n",
      "Detected 10 object(s); \n",
      "with scores: tensor([0.1710, 0.1689, 0.1600, 0.1597, 0.1597, 0.1597, 0.1597, 0.1596, 0.1596,\n",
      "        0.1596], grad_fn=<SliceBackward>); and \n",
      "with predicted classes: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check out the individual detections \n",
    "# note that the model has not been trained so the output is not meaningful but only displayed for mechanical check\n",
    "for i in range(image_batch.size(0)):\n",
    "    print(f\"For image {i}:\")\n",
    "    print(f\"Detected {detected_boxes[i].size(0)} object(s); \")\n",
    "    print(f\"with scores: {detected_scores[i]}; and \")\n",
    "    print(f\"with predicted classes: {detected_labels[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
