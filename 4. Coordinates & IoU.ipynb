{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from dataset import CocoDataset\n",
    "from utils   import *\n",
    "from model   import SSD300\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n",
      "image batch tensor shape: torch.Size([8, 3, 300, 300])\n",
      "bounding box location prediction shape: torch.Size([8, 8732, 4])\n",
      "object class prediction shape: torch.Size([8, 8732, 80])\n"
     ]
    }
   ],
   "source": [
    "# define the sequence of transformations to apply to each image sample \n",
    "basic_tfs = [PhotometricDistort(1.),\n",
    "             Flip(0.5),\n",
    "             ImageToTensor(), CategoryToTensor(), BoxToTensor(),\n",
    "             Zoomout(0.5, max_scale=2.5),\n",
    "             Normalize(), \n",
    "             Resize((300,300))]\n",
    "tfms = transforms.Compose(basic_tfs)\n",
    "\n",
    "# instantiate the dataset object\n",
    "ds = CocoDataset(data_dir='./', dataset='val2017', anno_type='instances', transforms=tfms)\n",
    "\n",
    "# create dataloader\n",
    "BS = 8\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True, \n",
    "                collate_fn=partial(ds.collate_fn, img_resized=True)) # img_resized=true to indicate all image samples have been resized to same shape\n",
    "\n",
    "# create model object\n",
    "ssd = SSD300(len(ds.allcats))\n",
    "\n",
    "# test forward pass for one batch\n",
    "for batch in dl:\n",
    "    image_batch = batch['images']\n",
    "    print(f\"image batch tensor shape: {image_batch.size()}\")\n",
    "    # forward pass through SSD300\n",
    "    locs, cls_scores = ssd(image_batch)\n",
    "    print(f\"bounding box location prediction shape: {locs.size()}\")\n",
    "    print(f\"object class prediction shape: {cls_scores.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Coordinate Transformations\n",
    "\n",
    "The native coordinate system for the COCO dataset for the bounding boxes are expressed in terms of $(x, y, w, h)$, where $(x, y)$ coordinates are measured from the top left image corner $(0, 0)$. We introduce three sets of coordinate systems that are utilized through the model prediction / optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO coordinates to center coordinates\n",
    "This transformation ecodes/decodes the original COCO bounding box coordinates $(x, y, w, h)$ (where $(x, y)$ represent the top-left corner of bounding box) to center coordinates $(x_c, y_c, w_c, h_c)$ where $(x_c, y_c)$ represent the center of the bounding box, furthermore, both $(x_c, y_c)$ and $(w_c, h_c)$ are normalized with respect to the original size of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coco2CenterCoord():\n",
    "    \"\"\"\n",
    "    Encodes/Decodes original COCO bounding box coordinates (x, y, w, h) where (x, y)\n",
    "    represent the top-left corner of bounding box (in image coordinate frame) to center \n",
    "    coordinates (x_c, y_c, w_c, h_c) where (x_c, y_c) represent the center of the bounding box, \n",
    "    furthermore, both (x_c, y_c) and (w_c, h_c) are normalized with respect to the original \n",
    "    size of image\n",
    "    \"\"\"       \n",
    "    def __init__(self, w, h):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        \n",
    "    def encode(self, boxes):\n",
    "        \"\"\"\n",
    "        boxes: bounding boxes tensor with coordinates in original COCO (x, y, w, h) format\n",
    "        \"\"\"\n",
    "        x_c = (boxes[:,0] + boxes[:,2]/2.0)/self.w\n",
    "        y_c = (boxes[:,1] + boxes[:,3]/2.0)/self.h\n",
    "        w_c = boxes[:,2]/self.w\n",
    "        h_c = boxes[:,3]/self.h\n",
    "        coords = [x_c, y_c, w_c, h_c]        \n",
    "        return torch.cat([c.unsqueeze(-1) for c in coords], dim=-1)\n",
    "    \n",
    "    def decode(self, boxes_c):\n",
    "        \"\"\"\n",
    "        boxes_c: bounding boxes tensor with coordinates in center coordinates (x_c, y_c, w_c, h_c) format\n",
    "        \"\"\"\n",
    "        x = (boxes_c[:,0] - boxes_c[:,2]/2.0) * self.w\n",
    "        y = (boxes_c[:,1] - boxes_c[:,3]/2.0) * self.h\n",
    "        width  = boxes_c[:,2] * self.w\n",
    "        height = boxes_c[:,3] * self.h\n",
    "        coords = [x, y, width, height]        \n",
    "        return torch.cat([c.unsqueeze(-1) for c in coords], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single dataset sample\n",
    "sample = ds[0]\n",
    "_, h, w = sample['image'].size()\n",
    "boxes_before = sample['boxes']\n",
    "\n",
    "# instantiate transform\n",
    "ccoord = Coco2CenterCoord(w, h)\n",
    "\n",
    "# transform box coordinates\n",
    "boxes_after = ccoord.encode(boxes_before)\n",
    "\n",
    "# inverse transform\n",
    "boxes_inverse = ccoord.decode(boxes_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box cooridnates before transformation:\n",
      " tensor([[156.8133, 126.4061,   6.0146,  25.4268],\n",
      "        [100.8190, 135.6439,  36.3604,  34.7085],\n",
      "        [234.7914, 150.8012,  19.8093,  28.8037]])\n",
      "\n",
      "box coordinates after transformation:\n",
      " tensor([[0.5327, 0.4637, 0.0200, 0.0848],\n",
      "        [0.3967, 0.5100, 0.1212, 0.1157],\n",
      "        [0.8157, 0.5507, 0.0660, 0.0960]])\n",
      "\n",
      "box coordinates apply inverse transformation:\n",
      " tensor([[156.8133, 126.4061,   6.0146,  25.4268],\n",
      "        [100.8190, 135.6439,  36.3604,  34.7085],\n",
      "        [234.7914, 150.8012,  19.8093,  28.8037]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"box cooridnates before transformation:\\n\", boxes_before[:3,:]);\n",
    "print(f\"\\nbox coordinates after transformation:\\n\", boxes_after[:3,:]);\n",
    "print(f\"\\nbox coordinates apply inverse transformation:\\n\", boxes_inverse[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center coordinates to Bounding Box coordinate offsets\n",
    "\n",
    "For the localization aspect of SSD prediction, the model **predicts** the *\"offsets relative to the default box shapes in the cell\"* at each of the feature map grid locations.\n",
    "\n",
    "- For the bounding box center coordinates $(x_c, y_c)$ of $(x_c, y_c, w_c, h_c)$ relative to prior box coordinates $(x_p, y_p, w_p, h_p)$, express offset $({\\Delta}x_c, {\\Delta}y_c) = (\\frac{(x_c - x_p)}{w_p}, \\frac{(y_c - y_p)}{h_p})$; and\n",
    "\n",
    "- For the bounding box shape coordinates $(w_c, h_c)$, express the shape offset $({\\Delta}w_c, {\\Delta}h_c) = (\\log{(\\frac{w_c}{w_p})}, \\log{(\\frac{h_c}{h_p})})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffsetCoord():\n",
    "    \"\"\"\n",
    "    Encodes/decodes the center coordinates (x_c, y_c, w_c, h_c) of bounding boxes relative to the prior \n",
    "    boxes (from SSD, expressed also in center coordinates) in terms of offset coordinates. This offset \n",
    "    coordinates is the form that is output by the SSD locator prediction. The offset coordinates have \n",
    "    the following relation:\n",
    "    (dx, dy) = ((x_c - x_p)/(x_p/10), (y_c - y_p)/(y_p/10)); and \n",
    "    (dw, dh) = (log(w_c/(w_p*5)), log(h_c/(h_p*5)))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def encode(self, cxcy, priors_cxcy):\n",
    "        \"\"\"\n",
    "        cxcy: bounding box in center-coordinate format\n",
    "        prior_cxcy: prior box in center-coordinate format\n",
    "        \"\"\"\n",
    "        dxdy = (cxcy[:,:2] - priors_cxcy[:,:2]) / (priors_cxcy[:,2:] / 10)\n",
    "        dwdh = torch.log(cxcy[:,2:] / priors_cxcy[:,2:]) * 5\n",
    "        return torch.cat([dxdy, dwdh], dim=1)\n",
    "    \n",
    "    \n",
    "    def decode(self, dxdy, priors_cxcy):\n",
    "        \"\"\"\n",
    "        dxdy: bounding boxes in offset-coordinate format wrt SSD's prior bounding boxes\n",
    "        \"\"\"\n",
    "        cxcy = dxdy[:,:2] * priors_cxcy[:,2:] / 10 + priors_cxcy[:,:2]\n",
    "        cwch = torch.exp(dxdy[:,2:] / 5) * priors_cxcy[:,2:]\n",
    "        return torch.cat([cxcy, cwch], dim=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding box in center coordinates:\n",
      "tensor([[0.5327, 0.4637, 0.0200, 0.0848]])\n",
      "\n",
      "bounding box in offset coordinates:\n",
      "tensor([[15975.7461, 13905.6348,    20.4839,    27.6920]])\n",
      "\n",
      "bounding box converted back to center coordinates:\n",
      "tensor([[0.5327, 0.4637, 0.0200, 0.0848]])\n"
     ]
    }
   ],
   "source": [
    "# encode SSD prior bounding boxes in center coords\n",
    "prior_boxes = ccoord.encode(ssd.prior_boxes)\n",
    "\n",
    "# init offset coord object\n",
    "ocoord = OffsetCoord()\n",
    "\n",
    "# select one bounding box location (already encoded in center coordinate format)\n",
    "bbox = boxes_after[:1,:]\n",
    "# select one prior box location (also encoded in center coordinate format)\n",
    "pbox = prior_boxes[:1,:]\n",
    "\n",
    "# pick one location prediction for demonstration purpose\n",
    "# encode the locational prediction output in offset coordinates, related to SSD prior bounding boxes\n",
    "bbox_offset = ocoord.encode(bbox, pbox)\n",
    "bbox_offset_inv = ocoord.decode(bbox_offset, pbox)\n",
    "\n",
    "print(f\"bounding box in center coordinates:\\n{bbox}\\n\")\n",
    "print(f\"bounding box in offset coordinates:\\n{bbox_offset}\\n\")\n",
    "print(f\"bounding box converted back to center coordinates:\\n{bbox_offset_inv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Over Union (IoU)\n",
    "\n",
    "The IoU is a simple concept that compute the \"intersection over union\" of two bounding box regions `b1` and `b2`. The union is computed as the sum of areas of the two boxes together minus the overlaping area (intersection) of the two boxes. THe IoU is then simply computed as a ratio of $\\frac{b1 \\cap b2}{b1 \\cup b2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
